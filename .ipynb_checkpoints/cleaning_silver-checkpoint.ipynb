{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88221692-c596-4829-a723-59767703781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03_cleaning_silver.ipynb\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType, TimestampType\n",
    "spark = SparkSession.builder.appName(\"Cleaning_Silver\").getOrCreate()\n",
    "\n",
    "# Paths (adapter si local)\n",
    "RAW_NOAA = \"hdfs://namenode:8020/raw/noaa/\"\n",
    "SILVER_NOAA = \"hdfs://namenode:8020/silver/noaa/\"\n",
    "\n",
    "RAW_USGS = \"hdfs://namenode:8020/raw/usgs/events/\"\n",
    "SILVER_USGS = \"hdfs://namenode:8020/silver/usgs/\"\n",
    "\n",
    "# -------- NOAA (batch) ----------\n",
    "\n",
    "# Lecture (support CSV and parquet)\n",
    "try:\n",
    "    df_noaa = spark.read.option(\"header\", True).csv(RAW_NOAA)  # si CSV\n",
    "except:\n",
    "    df_noaa = spark.read.parquet(RAW_NOAA)  # si parquet\n",
    "\n",
    "# Exemple de colonnes possibles -> adapter selon ton fichier NOAA\n",
    "# Normalisation : rename, types, unités\n",
    "# On crée un mapping standard : date, station_id, city, country, temp_c, wind_ms, precip_mm, lat, lon\n",
    "\n",
    "# helper conversion function if needed (°F -> °C)\n",
    "def f_to_c(col):\n",
    "    return (F.col(col) - F.lit(32.0)) * F.lit(5.0/9.0)\n",
    "\n",
    "# Exemple flexible : chercher colonnes candidates\n",
    "cols = df_noaa.columns\n",
    "# On fait plusieurs tentatives pour trouver température\n",
    "if \"TEMP\" in cols:\n",
    "    df_noaa = df_noaa.withColumn(\"temp_c\", F.col(\"TEMP\").cast(DoubleType()))\n",
    "elif \"tmax\" in cols:\n",
    "    df_noaa = df_noaa.withColumn(\"temp_c\", F.col(\"tmax\").cast(DoubleType()))\n",
    "# ... (ajoute d'autres cas selon ton CSV)\n",
    "\n",
    "# Pour un dataset générique, convertissons en types et supprimons lignes sans date ou without temp\n",
    "if \"DATE\" in cols:\n",
    "    df_noaa = df_noaa.withColumn(\"date\", F.to_timestamp(\"DATE\"))\n",
    "elif \"DATE_TIME\" in cols:\n",
    "    df_noaa = df_noaa.withColumn(\"date\", F.to_timestamp(\"DATE_TIME\"))\n",
    "\n",
    "# Cast columns safely (exemples)\n",
    "for c in [\"temp_c\", \"WIND\", \"PRCP\", \"LATITUDE\", \"LONGITUDE\"]:\n",
    "    if c in df_noaa.columns:\n",
    "        df_noaa = df_noaa.withColumn(c, F.col(c).cast(DoubleType()))\n",
    "\n",
    "# Fill or drop missing temperature rows (keep raw is stored already)\n",
    "df_noaa = df_noaa.filter(F.col(\"date\").isNotNull())\n",
    "\n",
    "# Standardize column names (adapt to your raw schema)\n",
    "df_noaa = df_noaa.withColumnRenamed(\"LATITUDE\", \"latitude\").withColumnRenamed(\"LONGITUDE\", \"longitude\")\n",
    "\n",
    "# If temp in F, convert\n",
    "if \"temp_f\" in df_noaa.columns:\n",
    "    df_noaa = df_noaa.withColumn(\"temp_c\", f_to_c(\"temp_f\"))\n",
    "\n",
    "# Create canonical columns if missing (placeholders)\n",
    "if \"temp_c\" not in df_noaa.columns:\n",
    "    df_noaa = df_noaa.withColumn(\"temp_c\", F.lit(None).cast(DoubleType()))\n",
    "\n",
    "# Write SILVER\n",
    "df_noaa.select(\"date\", \"station_id\", \"latitude\", \"longitude\", \"temp_c\").write.mode(\"overwrite\").partitionBy(F.year(\"date\").alias(\"year\")).parquet(SILVER_NOAA)\n",
    "\n",
    "print(\"NOAA -> SILVER saved\")\n",
    "\n",
    "# -------- USGS cleaning ----------\n",
    "df_usgs = spark.read.parquet(RAW_USGS)  # reading event parquet written earlier\n",
    "\n",
    "# Ensure columns exist and types are correct\n",
    "df_usgs = df_usgs.withColumn(\"magnitude\", F.col(\"mag\").cast(DoubleType())) \\\n",
    "                 .withColumn(\"event_time\", F.col(\"event_time\").cast(TimestampType())) \\\n",
    "                 .withColumn(\"latitude\", F.col(\"latitude\").cast(DoubleType())) \\\n",
    "                 .withColumn(\"longitude\", F.col(\"longitude\").cast(DoubleType())) \\\n",
    "                 .withColumn(\"place\", F.col(\"place\"))\n",
    "\n",
    "# Drop records without lat/lon\n",
    "df_usgs = df_usgs.filter(F.col(\"latitude\").isNotNull() & F.col(\"longitude\").isNotNull())\n",
    "\n",
    "# Add human readable date\n",
    "df_usgs = df_usgs.withColumn(\"date\", F.to_date(\"event_time\"))\n",
    "\n",
    "# Write SILVER\n",
    "df_usgs.write.mode(\"append\").partitionBy(\"date\").parquet(SILVER_USGS)\n",
    "print(\"USGS -> SILVER saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
