{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f512a40b-b281-44ca-924f-99bbd7fce7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02_ingestion_usgs.ipynb\n",
    "from pyspark.sql import SparkSession\n",
    "import requests, json, time, os\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialiser la session Spark\n",
    "spark = SparkSession.builder.appName(\"USGS_Ingestion\").getOrCreate()\n",
    "\n",
    "# Définir le chemin HDFS - ESSAYER CES OPTIONS DANS L'ORDRE :\n",
    "# Option 1: localhost (si HDFS est en local)\n",
    "RAW_BASE = \"hdfs://localhost:9000/raw/usgs\"\n",
    "\n",
    "# Option 2: Adresse IP directe (si vous connaissez l'IP de namenode)\n",
    "# RAW_BASE = \"hdfs://192.168.1.100:8020/raw/usgs\"\n",
    "\n",
    "# Option 3: file:// pour système de fichiers local (fallback)\n",
    "# RAW_BASE = \"file:///tmp/raw/usgs\"\n",
    "\n",
    "# Créer le répertoire temporaire local dans le répertoire utilisateur actuel\n",
    "LOCAL_TEMP_DIR = \"./usgs_temp\"\n",
    "os.makedirs(LOCAL_TEMP_DIR, exist_ok=True)\n",
    "\n",
    "def test_hdfs_connection():\n",
    "    \"\"\"Tester la connexion HDFS et trouver la bonne configuration\"\"\"\n",
    "    test_paths = [\n",
    "        \"hdfs://localhost:9000\",\n",
    "        \"hdfs://127.0.0.1:9000\", \n",
    "        \"hdfs://namenode:8020\",\n",
    "        \"hdfs://0.0.0.0:9000\",\n",
    "        \"file:///tmp\"\n",
    "    ]\n",
    "    \n",
    "    for path in test_paths:\n",
    "        try:\n",
    "            test_dir = f\"{path}/test_connection\"\n",
    "            spark.sparkContext.parallelize([1]).saveAsTextFile(test_dir)\n",
    "            print(f\"✓ Connexion réussie: {path}\")\n",
    "            # Nettoyer\n",
    "            fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "            fs.delete(spark._jvm.org.apache.hadoop.fs.Path(test_dir), True)\n",
    "            return path\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Échec {path}: {str(e)[:100]}...\")\n",
    "    \n",
    "    print(\"⚠ Aucune connexion HDFS trouvée, utilisation du système de fichiers local\")\n",
    "    return \"file:///tmp\"\n",
    "\n",
    "def fetch_and_save(feed_url=\"https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_hour.geojson\"):\n",
    "    \"\"\"\n",
    "    Récupère les données sismiques de l'USGS et les sauvegarde\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Récupérer les données depuis l'API USGS\n",
    "        print(\"Récupération des données depuis l'API USGS...\")\n",
    "        r = requests.get(feed_url, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        \n",
    "        # Créer un fichier local avec timestamp\n",
    "        timestamp = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "        local_path = f\"{LOCAL_TEMP_DIR}/earthquakes_{timestamp}.json\"\n",
    "        \n",
    "        # Sauvegarder les données en local temporairement\n",
    "        with open(local_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f)\n",
    "        \n",
    "        print(f\"Données brutes sauvegardées localement: {local_path}\")\n",
    "        \n",
    "        # Lire le JSON avec Spark\n",
    "        print(\"Lecture des données avec Spark...\")\n",
    "        df = spark.read.json(local_path)\n",
    "        \n",
    "        # Vérifier si des données sont présentes\n",
    "        if df.count() == 0:\n",
    "            print(\"Aucune donnée à traiter\")\n",
    "            os.remove(local_path)\n",
    "            return None\n",
    "            \n",
    "        # Normaliser la structure: features -> properties + geometry\n",
    "        print(\"Transformation des données...\")\n",
    "        df2 = df.selectExpr(\"explode(features) as feat\").selectExpr(\n",
    "            \"feat.properties.*\",\n",
    "            \"feat.geometry.coordinates as coords\",\n",
    "            \"feat.id as event_id\"\n",
    "        ).withColumnRenamed(\"time\", \"time_ms\")\n",
    "        \n",
    "        # Convertir les colonnes\n",
    "        import pyspark.sql.functions as F\n",
    "        df2 = df2.withColumn(\"event_time\", (F.col(\"time_ms\")/1000).cast(\"timestamp\")) \\\n",
    "                 .withColumn(\"longitude\", F.col(\"coords\").getItem(0)) \\\n",
    "                 .withColumn(\"latitude\", F.col(\"coords\").getItem(1)) \\\n",
    "                 .withColumn(\"depth_km\", F.col(\"coords\").getItem(2))\n",
    "        \n",
    "        # Afficher le schéma et un échantillon\n",
    "        print(\"Schéma des données transformées:\")\n",
    "        df2.printSchema()\n",
    "        \n",
    "        event_count = df2.count()\n",
    "        print(f\"Nombre d'événements à sauvegarder: {event_count}\")\n",
    "        \n",
    "        if event_count > 0:\n",
    "            df2.show(3, truncate=False)\n",
    "        \n",
    "        # Créer le chemin de sortie avec partitionnement par date\n",
    "        date_partition = datetime.utcnow().strftime('%Y/%m/%d')\n",
    "        out_path = f\"{RAW_BASE}/events/{date_partition}\"\n",
    "        \n",
    "        # Sauvegarder en format Parquet\n",
    "        print(f\"Sauvegarde dans: {out_path}\")\n",
    "        df2.write.mode(\"append\").parquet(out_path)\n",
    "        print(f\"✓ Données sauvegardées avec succès: {out_path}\")\n",
    "        \n",
    "        # Nettoyer le fichier local temporaire\n",
    "        os.remove(local_path)\n",
    "        print(f\"✓ Fichier local temporaire supprimé\")\n",
    "        \n",
    "        return out_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur lors de l'ingestion: {e}\")\n",
    "        # Essayer de sauvegarder localement en fallback\n",
    "        try:\n",
    "            if 'local_path' in locals():\n",
    "                backup_path = f\"{LOCAL_TEMP_DIR}/backup_earthquakes_{timestamp}.json\"\n",
    "                os.rename(local_path, backup_path)\n",
    "                print(f\"✓ Données sauvegardées en local: {backup_path}\")\n",
    "        except:\n",
    "            pass\n",
    "        return None\n",
    "\n",
    "# Tester la connexion HDFS au démarrage\n",
    "print(\"=== Test de connexion HDFS ===\")\n",
    "BASE_PATH = test_hdfs_connection()\n",
    "RAW_BASE = f\"{BASE_PATH}/raw/usgs\"\n",
    "print(f\"Chemin HDFS utilisé: {RAW_BASE}\")\n",
    "\n",
    "# Point d'entrée principal\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n=== Démarrage de l'ingestion USGS ===\")\n",
    "    \n",
    "    for i in range(3):  # À ajuster/supprimer en production\n",
    "        print(f\"\\n--- Itération {i+1}/3 ---\")\n",
    "        try:\n",
    "            result = fetch_and_save()\n",
    "            if result:\n",
    "                print(f\"✓ Itération {i+1}/3 terminée avec succès\")\n",
    "            else:\n",
    "                print(f\"⚠ Itération {i+1}/3 échouée ou aucune donnée\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erreur lors de l'itération {i+1}: {e}\")\n",
    "        \n",
    "        # Pause de 10 secondes entre chaque extraction (pour tests)\n",
    "        if i < 2:\n",
    "            print(\"⏳ Attente de 10 secondes...\")\n",
    "            time.sleep(10)\n",
    "    \n",
    "    print(\"\\n=== Ingestion USGS terminée ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5d3827-97a5-4d3f-9d03-434d10b0ccd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
