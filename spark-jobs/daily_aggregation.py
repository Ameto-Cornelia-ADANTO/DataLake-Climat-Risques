#!/usr/bin/env python3
"""
Job Spark pour l'agr√©gation quotidienne des donn√©es
Cr√©e des vues agr√©g√©es pour le dashboard
"""
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, avg, max, min, sum, date_format
from pyspark.sql.functions import when, year, month, dayofmonth
import logging
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def init_spark():
    """Initialise la session Spark"""
    return SparkSession.builder \
        .appName("Daily_Aggregation_Job") \
        .config("spark.hadoop.fs.defaultFS", "hdfs://namenode:9000") \
        .config("spark.sql.parquet.compression.codec", "snappy") \
        .getOrCreate()

def aggregate_noaa_daily(spark):
    """Agr√®ge les donn√©es NOAA par jour et station"""
    logger.info("üìä Agr√©gation quotidienne NOAA...")
    
    # Lire les donn√©es nettoy√©es
    silver_path = "hdfs://namenode:9000/hadoop-climate-risk/silver/"
    try:
        df_noaa = spark.read.parquet(f"{silver_path}/noaa_cleaned_*")
        logger.info(f"‚úÖ Donn√©es NOAA charg√©es: {df_noaa.count()} lignes")
    except:
        logger.warning("‚ö†Ô∏è Aucune donn√©e NOAA nettoy√©e trouv√©e")
        return None
    
    # Agr√©gation par jour et station
    df_daily = df_noaa.groupBy("date", "station_id", "year", "month", "day").agg(
        avg("max_temp").alias("avg_max_temp"),
        avg("min_temp").alias("avg_min_temp"),
        avg("avg_temp").alias("avg_daily_temp"),
        sum("precipitation").alias("total_precipitation"),
        sum("snow").alias("total_snow"),
        avg("avg_wind_speed").alias("avg_wind_speed"),
        count("*").alias("measurement_count")
    )
    
    # Ajouter des indicateurs
    df_daily = df_daily.withColumn(
        "precipitation_category",
        when(col("total_precipitation") == 0, "Sec")
        .when(col("total_precipitation") < 5, "L√©ger")
        .when(col("total_precipitation") < 20, "Mod√©r√©")
        .otherwise("Fort")
    )
    
    logger.info(f"‚úÖ NOAA agr√©g√©: {df_daily.count()} lignes journali√®res")
    return df_daily

def aggregate_usgs_daily(spark):
    """Agr√®ge les donn√©es USGS par jour"""
    logger.info("üìä Agr√©gation quotidienne USGS...")
    
    # Lire les donn√©es nettoy√©es
    silver_path = "hdfs://namenode:9000/hadoop-climate-risk/silver/"
    try:
        df_usgs = spark.read.parquet(f"{silver_path}/usgs_cleaned_*")
        logger.info(f"‚úÖ Donn√©es USGS charg√©es: {df_usgs.count()} lignes")
    except:
        logger.warning("‚ö†Ô∏è Aucune donn√©e USGS nettoy√©e trouv√©e")
        return None
    
    # Agr√©gation par jour
    df_daily = df_usgs.groupBy(
        date_format(col("timestamp"), "yyyy-MM-dd").alias("date"),
        "year", "month"
    ).agg(
        count("*").alias("earthquake_count"),
        avg("magnitude").alias("avg_magnitude"),
        max("magnitude").alias("max_magnitude"),
        min("magnitude").alias("min_magnitude"),
        avg("depth").alias("avg_depth"),
        count(when(col("severity") >= 3, True)).alias("significant_quakes")
    )
    
    # Ajouter des indicateurs
    df_daily = df_daily.withColumn(
        "activity_level",
        when(col("earthquake_count") == 0, "Calme")
        .when(col("earthquake_count") < 5, "Faible")
        .when(col("earthquake_count") < 10, "Mod√©r√©")
        .otherwise("√âlev√©")
    )
    
    logger.info(f"‚úÖ USGS agr√©g√©: {df_daily.count()} jours d'activit√© sismique")
    return df_daily

def create_cross_aggregation(spark, df_noaa_daily, df_usgs_daily):
    """Cr√©e une agr√©gation crois√©e NOAA-USGS"""
    if df_noaa_daily is None or df_usgs_daily is None:
        return None
    
    logger.info("üîÑ Cr√©ation agr√©gation crois√©e...")
    
    # Pr√©parer les donn√©es pour la jointure
    noaa_prep = df_noaa_daily.groupBy("date").agg(
        avg("avg_daily_temp").alias("avg_temp_all_stations"),
        max("avg_max_temp").alias("max_temp_day"),
        min("avg_min_temp").alias("min_temp_day"),
        avg("total_precipitation").alias("avg_precipitation")
    )
    
    usgs_prep = df_usgs_daily.select(
        col("date"),
        col("earthquake_count"),
        col("avg_magnitude"),
        col("activity_level")
    )
    
    # Jointure
    df_cross = noaa_prep.join(usgs_prep, "date", "outer")
    
    # Remplir les valeurs manquantes
    df_cross = df_cross.fillna({
        "earthquake_count": 0,
        "avg_magnitude": 0,
        "activity_level": "Calme",
        "avg_temp_all_stations": 0,
        "max_temp_day": 0,
        "min_temp_day": 0,
        "avg_precipitation": 0
    })
    
    # Ajouter une colonne de corr√©lation
    df_cross = df_cross.withColumn(
        "weather_seismic_correlation",
        when((col("earthquake_count") > 5) & (col("avg_temp_all_stations") > 25), "Possible")
        .when((col("earthquake_count") > 0) & (col("max_temp_day") - col("min_temp_day") > 15), "√Ä √©tudier")
        .otherwise("Non d√©tect√©e")
    )
    
    logger.info(f"‚úÖ Agr√©gation crois√©e: {df_cross.count()} jours")
    return df_cross

def save_aggregations(spark, df_noaa_daily, df_usgs_daily, df_cross):
    """Sauvegarde toutes les agr√©gations dans GOLD"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    gold_path = "hdfs://namenode:9000/hadoop-climate-risk/gold"
    
    # Sauvegarder NOAA daily
    if df_noaa_daily is not None:
        noaa_gold = f"{gold_path}/noaa_daily_agg_{timestamp}"
        df_noaa_daily.write \
            .mode("overwrite") \
            .partitionBy("year", "month") \
            .parquet(noaa_gold)
        logger.info(f"üíæ NOAA daily sauvegard√©: {noaa_gold}")
    
    # Sauvegarder USGS daily
    if df_usgs_daily is not None:
        usgs_gold = f"{gold_path}/usgs_daily_agg_{timestamp}"
        df_usgs_daily.write \
            .mode("overwrite") \
            .partitionBy("year", "month") \
            .parquet(usgs_gold)
        logger.info(f"üíæ USGS daily sauvegard√©: {usgs_gold}")
    
    # Sauvegarder cross aggregation
    if df_cross is not None:
        cross_gold = f"{gold_path}/cross_daily_agg_{timestamp}"
        df_cross.write \
            .mode("overwrite") \
            .parquet(cross_gold)
        logger.info(f"üíæ Cross aggregation sauvegard√©: {cross_gold}")

def main():
    """Fonction principale"""
    logger.info("üöÄ D√©marrage job Daily Aggregation")
    
    # Initialiser Spark
    spark = init_spark()
    
    try:
        # Cr√©er les agr√©gations
        df_noaa_daily = aggregate_noaa_daily(spark)
        df_usgs_daily = aggregate_usgs_daily(spark)
        df_cross = create_cross_aggregation(spark, df_noaa_daily, df_usgs_daily)
        
        # Sauvegarder
        save_aggregations(spark, df_noaa_daily, df_usgs_daily, df_cross)
        
        # R√©sum√©
        logger.info("=" * 50)
        logger.info("üìã R√âSUM√â DAILY AGGREGATION")
        if df_noaa_daily:
            logger.info(f"   NOAA daily: {df_noaa_daily.count()} lignes")
        if df_usgs_daily:
            logger.info(f"   USGS daily: {df_usgs_daily.count()} lignes")
        if df_cross:
            logger.info(f"   Cross aggregation: {df_cross.count()} lignes")
        logger.info("‚úÖ Job Daily Aggregation termin√© avec succ√®s")
        logger.info("=" * 50)
        
    except Exception as e:
        logger.error(f"‚ùå Erreur dans le job d'agr√©gation: {e}")
        raise
    finally:
        spark.stop()

if __name__ == "__main__":
    main()