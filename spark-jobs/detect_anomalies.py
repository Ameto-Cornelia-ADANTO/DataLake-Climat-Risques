#!/usr/bin/env python3
"""
Job Spark pour la dÃ©tection d'anomalies dans les donnÃ©es climatiques et sismiques
"""
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg, stddev, when, abs, lit
from pyspark.sql.window import Window
import logging
from datetime import datetime
import numpy as np

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def init_spark():
    """Initialise la session Spark"""
    return SparkSession.builder \
        .appName("Anomaly_Detection_Job") \
        .config("spark.hadoop.fs.defaultFS", "hdfs://namenode:9000") \
        .config("spark.sql.parquet.compression.codec", "snappy") \
        .getOrCreate()

def detect_temperature_anomalies(spark):
    """DÃ©tecte les anomalies de tempÃ©rature"""
    logger.info("ğŸŒ¡ï¸ DÃ©tection anomalies tempÃ©rature...")
    
    # Lire les donnÃ©es NOAA nettoyÃ©es
    try:
        df_noaa = spark.read.parquet("hdfs://namenode:9000/hadoop-climate-risk/silver/noaa_cleaned_*")
        logger.info(f"âœ… DonnÃ©es NOAA chargÃ©es: {df_noaa.count()} lignes")
    except:
        logger.warning("âš ï¸ Aucune donnÃ©e NOAA trouvÃ©e")
        return None
    
    # Calculer les statistiques par station
    window_spec = Window.partitionBy("station_id").orderBy("date").rowsBetween(-30, 0)
    
    df_anomalies = df_noaa.withColumn("avg_temp_30d", avg("avg_temp").over(window_spec)) \
        .withColumn("std_temp_30d", stddev("avg_temp").over(window_spec)) \
        .withColumn("z_score", 
            when(col("std_temp_30d") != 0, 
                abs((col("avg_temp") - col("avg_temp_30d")) / col("std_temp_30d"))
            ).otherwise(0)
        ) \
        .withColumn("is_anomaly", col("z_score") > 3) \
        .withColumn("anomaly_type",
            when((col("avg_temp") - col("avg_temp_30d")) > 0, "High")
            .otherwise("Low")
        )
    
    # Filtrer seulement les anomalies
    df_detected = df_anomalies.filter(col("is_anomaly") == True) \
        .select(
            "date", "station_id", "avg_temp", "avg_temp_30d",
            "z_score", "anomaly_type", "latitude", "longitude"
        )
    
    logger.info(f"âœ… {df_detected.count()} anomalies tempÃ©rature dÃ©tectÃ©es")
    return df_detected

def detect_precipitation_anomalies(spark):
    """DÃ©tecte les anomalies de prÃ©cipitations"""
    logger.info("ğŸŒ§ï¸ DÃ©tection anomalies prÃ©cipitations...")
    
    try:
        df_noaa = spark.read.parquet("hdfs://namenode:9000/hadoop-climate-risk/silver/noaa_cleaned_*")
    except:
        return None
    
    # Anomalies de prÃ©cipitations (valeur absolue > 50mm ou 0mm pendant 10 jours)
    window_spec = Window.partitionBy("station_id").orderBy("date").rowsBetween(-10, 0)
    
    df_anomalies = df_noaa.withColumn("avg_precip_10d", avg("precipitation").over(window_spec)) \
        .withColumn("is_dry_spell", 
            when((col("precipitation") == 0) & (col("avg_precip_10d") == 0), True)
            .otherwise(False)
        ) \
        .withColumn("is_flood_risk", col("precipitation") > 50) \
        .filter((col("is_dry_spell") == True) | (col("is_flood_risk") == True)) \
        .withColumn("precip_anomaly_type",
            when(col("is_dry_spell") == True, "SÃ©cheresse")
            .when(col("is_flood_risk") == True, "Risque inondation")
            .otherwise("Normal")
        )
    
    df_detected = df_anomalies.select(
        "date", "station_id", "precipitation", "avg_precip_10d",
        "precip_anomaly_type", "latitude", "longitude"
    )
    
    logger.info(f"âœ… {df_detected.count()} anomalies prÃ©cipitations dÃ©tectÃ©es")
    return df_detected

def detect_seismic_anomalies(spark):
    """DÃ©tecte les anomalies sismiques"""
    logger.info("ğŸŒ‹ DÃ©tection anomalies sismiques...")
    
    try:
        df_usgs = spark.read.parquet("hdfs://namenode:9000/hadoop-climate-risk/silver/usgs_cleaned_*")
        logger.info(f"âœ… DonnÃ©es USGS chargÃ©es: {df_usgs.count()} lignes")
    except:
        logger.warning("âš ï¸ Aucune donnÃ©e USGS trouvÃ©e")
        return None
    
    # DÃ©tection de clusters sismiques
    from pyspark.sql.functions import count, avg as spark_avg
    
    # AgrÃ©gation par rÃ©gion et jour
    df_region = df_usgs.groupBy(
        "year", "month", "day",
        when(col("latitude").between(30, 50) & col("longitude").between(-130, -60), "USA_West")
        .when(col("latitude").between(50, 70) & col("longitude").between(-180, -130), "Alaska")
        .when(col("latitude").between(18, 30) & col("longitude").between(-160, -154), "Hawaii")
        .otherwise("Autre").alias("region")
    ).agg(
        count("*").alias("quake_count"),
        spark_avg("magnitude").alias("avg_magnitude"),
        spark_avg("depth").alias("avg_depth")
    )
    
    # DÃ©tection d'anomalies (plus de 10 sÃ©ismes par jour dans une rÃ©gion)
    window_spec = Window.partitionBy("region").orderBy("year", "month", "day").rowsBetween(-7, 0)
    
    df_anomalies = df_region.withColumn("avg_quakes_7d", 
        spark_avg("quake_count").over(window_spec)) \
        .withColumn("is_seismic_swarm",
            when(col("quake_count") > 10, True)
            .when(col("quake_count") > (col("avg_quakes_7d") * 3), True)
            .otherwise(False)
        ) \
        .filter(col("is_seismic_swarm") == True)
    
    logger.info(f"âœ… {df_anomalies.count()} anomalies sismiques dÃ©tectÃ©es")
    return df_anomalies

def save_anomalies(spark, temp_anomalies, precip_anomalies, seismic_anomalies):
    """Sauvegarde les anomalies dÃ©tectÃ©es"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    gold_path = "hdfs://namenode:9000/hadoop-climate-risk/gold/anomalies"
    
    # Sauvegarder anomalies tempÃ©rature
    if temp_anomalies is not None and temp_anomalies.count() > 0:
        temp_path = f"{gold_path}/temperature_{timestamp}"
        temp_anomalies.write \
            .mode("overwrite") \
            .parquet(temp_path)
        logger.info(f"ğŸ’¾ Anomalies tempÃ©rature sauvegardÃ©es: {temp_path}")
    
    # Sauvegarder anomalies prÃ©cipitations
    if precip_anomalies is not None and precip_anomalies.count() > 0:
        precip_path = f"{gold_path}/precipitation_{timestamp}"
        precip_anomalies.write \
            .mode("overwrite") \
            .parquet(precip_path)
        logger.info(f"ğŸ’¾ Anomalies prÃ©cipitations sauvegardÃ©es: {precip_path}")
    
    # Sauvegarder anomalies sismiques
    if seismic_anomalies is not None and seismic_anomalies.count() > 0:
        seismic_path = f"{gold_path}/seismic_{timestamp}"
        seismic_anomalies.write \
            .mode("overwrite") \
            .partitionBy("region") \
            .parquet(seismic_path)
        logger.info(f"ğŸ’¾ Anomalies sismiques sauvegardÃ©es: {seismic_path}")

def main():
    """Fonction principale"""
    logger.info("ğŸš€ DÃ©marrage job Anomaly Detection")
    
    # Initialiser Spark
    spark = init_spark()
    
    try:
        # DÃ©tecter les anomalies
        temp_anomalies = detect_temperature_anomalies(spark)
        precip_anomalies = detect_precipitation_anomalies(spark)
        seismic_anomalies = detect_seismic_anomalies(spark)
        
        # Sauvegarder
        save_anomalies(spark, temp_anomalies, precip_anomalies, seismic_anomalies)
        
        # RÃ©sumÃ©
        logger.info("=" * 50)
        logger.info("ğŸ“‹ RÃ‰SUMÃ‰ ANOMALY DETECTION")
        if temp_anomalies:
            logger.info(f"   Anomalies tempÃ©rature: {temp_anomalies.count()}")
        if precip_anomalies:
            logger.info(f"   Anomalies prÃ©cipitations: {precip_anomalies.count()}")
        if seismic_anomalies:
            logger.info(f"   Anomalies sismiques: {seismic_anomalies.count()}")
        logger.info("âœ… Job Anomaly Detection terminÃ© avec succÃ¨s")
        logger.info("=" * 50)
        
    except Exception as e:
        logger.error(f"âŒ Erreur dans le job de dÃ©tection d'anomalies: {e}")
        raise
    finally:
        spark.stop()

if __name__ == "__main__":
    main()