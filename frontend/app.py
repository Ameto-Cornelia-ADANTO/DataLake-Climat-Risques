# ============================================
# DataLake Climat & Risques Naturels
# Dashboard Streamlit - Version ComplÃ¨te
# ============================================
import streamlit as st
import pandas as pd
import numpy as np

# DÃ©sactiver certains caches problÃ©matiques
st.set_option('client.caching', False)
import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
from datetime import datetime, timedelta
import time
import os

# ============================================
# CONFIGURATION DE LA PAGE
# ============================================

st.set_page_config(
    page_title="DataLake Climat & Risques Naturels",
    page_icon="ğŸŒ",
    layout="wide"
)

# ============================================
# STYLES CSS
# ============================================

st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1E88E5;
        text-align: center;
        padding: 1rem;
        background: linear-gradient(90deg, #1E88E5, #4CAF50);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        font-weight: bold;
        margin-bottom: 1rem;
    }
    .kpi-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1.5rem;
        border-radius: 15px;
        color: white;
        text-align: center;
        box-shadow: 0 6px 12px rgba(0,0,0,0.15);
        margin: 0.5rem;
        transition: transform 0.3s;
    }
    .kpi-card:hover {
        transform: translateY(-5px);
    }
    .kpi-card h2 {
        font-size: 2.2rem;
        margin: 0.5rem 0;
        font-weight: bold;
    }
    .kpi-card p {
        margin: 0;
        font-size: 0.9rem;
        opacity: 0.9;
    }
    .stTabs [data-baseweb="tab-list"] {
        gap: 8px;
    }
    .stTabs [data-baseweb="tab"] {
        height: 50px;
        white-space: pre-wrap;
        background-color: #f0f2f6;
        border-radius: 5px 5px 0px 0px;
        gap: 1px;
        padding-top: 10px;
        padding-bottom: 10px;
    }
    .stTabs [aria-selected="true"] {
        background-color: #4CAF50;
        color: white;
    }
</style>
""", unsafe_allow_html=True)

# ============================================
# TITRE PRINCIPAL
# ============================================

st.markdown('<h1 class="main-header">ğŸŒ DataLake Climat & Risques Naturels</h1>', unsafe_allow_html=True)
st.markdown("### **NOAA (MÃ©tÃ©o & Climat)** + **USGS (Risques Naturels)**")

# ============================================
# MENU LATÃ‰RAL
# ============================================

menu = st.sidebar.radio(
    "ğŸ“Š Navigation",
    ["ğŸ  Dashboard", "ğŸ“ˆ Visualisations", "ğŸš¨ Alertes", "ğŸ“ HDFS Explorer", "âš™ï¸ Administration", "ğŸ—ï¸ Architecture"]
)

# VÃ©rification des services Docker
with st.sidebar.expander("ğŸ”§ Ã‰tat des Services"):
    try:
        import docker
        client = docker.from_env()
        containers = client.containers.list()
        st.success(f"âœ… {len(containers)} conteneurs actifs")
        for container in containers[:5]:
            status = "ğŸŸ¢" if container.status == "running" else "ğŸŸ¡"
            st.write(f"{status} {container.name[:20]}...")
    except:
        st.info("â„¹ï¸ Docker Desktop non dÃ©tectÃ© (mode simulation)")

# ============================================
# DONNÃ‰ES SIMULÃ‰ES
# ============================================

@st.cache_data(ttl=300)  # Cache 5 minutes
def generate_data():
    """GÃ©nÃ¨re des donnÃ©es simulÃ©es pour le dashboard"""
    dates = pd.date_range('2024-01-01', periods=100)
    return pd.DataFrame({
        'date': dates,
        'temperature': np.random.normal(15, 5, 100).cumsum()/100 + 15,
        'precipitation': np.random.exponential(2, 100),
        'earthquakes': np.random.poisson(2, 100),
        'region': np.random.choice(['California', 'Alaska', 'Hawaii', 'Texas'], 100)
    })

# ============================================
# PAGE 1 : DASHBOARD PRINCIPAL
# ============================================

if menu == "ğŸ  Dashboard":
    
    # Titre
    st.header("ğŸ“Š Tableau de Bord Principal")
    
    # DonnÃ©es
    df = generate_data()
    
    # ========== KPIs ==========
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.markdown("""
        <div class="kpi-card">
            <p>ğŸŒ¡ï¸ TempÃ©rature</p>
            <h2>15.2Â°C</h2>
            <p>+0.5Â°C</p>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown("""
        <div class="kpi-card">
            <p>ğŸŒ§ï¸ PrÃ©cipitation</p>
            <h2>42 mm</h2>
            <p>-10%</p>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        st.markdown("""
        <div class="kpi-card">
            <p>ğŸŒ‹ SÃ©ismes</p>
            <h2>24</h2>
            <p>+3</p>
        </div>
        """, unsafe_allow_html=True)
    
    with col4:
        st.markdown("""
        <div class="kpi-card">
            <p>ğŸš¨ Alertes</p>
            <h2>2</h2>
            <p>Actives</p>
        </div>
        """, unsafe_allow_html=True)
    
    # ========== GRAPHIQUES ==========
    st.markdown("---")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # Graphique tempÃ©rature
        fig_temp = px.line(df, x='date', y='temperature', 
                          title='ğŸ“ˆ Ã‰volution des TempÃ©ratures',
                          labels={'temperature': 'TempÃ©rature (Â°C)', 'date': 'Date'},
                          line_shape='spline')
        fig_temp.update_layout(
            height=400,
            hovermode='x unified',
            plot_bgcolor='rgba(240, 242, 246, 0.8)'
        )
        st.plotly_chart(fig_temp, use_container_width=True)
    
    with col2:
        # Graphique activitÃ© sismique
        region_counts = df.groupby('region')['earthquakes'].sum().reset_index()
        fig_seismic = px.bar(region_counts,
                            x='region', y='earthquakes',
                            title='ğŸŒ‹ ActivitÃ© Sismique par RÃ©gion',
                            color='region',
                            color_discrete_sequence=px.colors.qualitative.Set3)
        fig_seismic.update_layout(
            height=400,
            xaxis_title="RÃ©gion",
            yaxis_title="Nombre de sÃ©ismes",
            plot_bgcolor='rgba(240, 242, 246, 0.8)'
        )
        st.plotly_chart(fig_seismic, use_container_width=True)
    
    # ========== CARTE ==========
    st.markdown("---")
    st.subheader("ğŸ“ Carte des Stations et SÃ©ismes")
    
    # DonnÃ©es pour la carte
    df_map = pd.DataFrame({
        'lat': np.random.uniform(30, 50, 20),
        'lon': np.random.uniform(-130, -60, 20),
        'type': np.random.choice(['Station NOAA', 'SÃ©isme USGS'], 20),
        'size': np.random.randint(10, 50, 20)
    })
    
    # Ajouter des couleurs au format hexadÃ©cimal
    df_map['color'] = df_map['type'].map({
        'Station NOAA': '#FF0000',  # Rouge
        'SÃ©isme USGS': '#0000FF'    # Bleu
    })
    
    # Carte simplifiÃ©e
    try:
        # Version simplifiÃ©e
        st.map(df_map[['lat', 'lon', 'color']].dropna())
        
        # LÃ©gende
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("ğŸ”´ **Stations NOAA** - DonnÃ©es mÃ©tÃ©orologiques")
        with col2:
            st.markdown("ğŸ”µ **SÃ©ismes USGS** - ActivitÃ© sismique")
            
    except Exception as e:
        st.error(f"Erreur avec la carte : {str(e)[:100]}")
        # Afficher les donnÃ©es Ã  la place
        st.dataframe(df_map.head(10))

# ============================================
# PAGE 2 : VISUALISATIONS
# ============================================

elif menu == "ğŸ“ˆ Visualisations":
    
    st.header("ğŸ“Š Visualisations AvancÃ©es")
    
    # Onglets
    tab1, tab2, tab3 = st.tabs(["ğŸ“Š NOAA", "ğŸŒ‹ USGS", "ğŸ”— CorrÃ©lations"])
    
    # ========== TAB 1 : NOAA ==========
    with tab1:
        st.subheader("DonnÃ©es MÃ©tÃ©orologiques NOAA")
        
        # DonnÃ©es NOAA simulÃ©es
        df_noaa = pd.DataFrame({
            'Station': ['New York', 'Los Angeles', 'Chicago', 'Miami'] * 25,
            'Temperature (Â°C)': np.random.normal(20, 10, 100),
            'Humidity (%)': np.random.uniform(30, 90, 100),
            'Wind Speed (km/h)': np.random.exponential(15, 100),
            'Pressure (hPa)': np.random.normal(1013, 10, 100)
        })
        
        col1, col2 = st.columns(2)
        
        with col1:
            # Scatter plot SANS trendline pour Ã©viter l'erreur statsmodels
            fig_scatter = px.scatter(df_noaa, 
                                    x='Temperature (Â°C)', 
                                    y='Humidity (%)', 
                                    color='Station', 
                                    size='Wind Speed (km/h)',
                                    title='TempÃ©rature vs HumiditÃ© par Station',
                                    hover_data=['Pressure (hPa)'])
            fig_scatter.update_layout(height=450)
            st.plotly_chart(fig_scatter, use_container_width=True)
        
        with col2:
            # Box plot
            fig_box = px.box(df_noaa, 
                            x='Station', 
                            y='Temperature (Â°C)',
                            title='Distribution des TempÃ©ratures par Station',
                            color='Station')
            fig_box.update_layout(height=450)
            st.plotly_chart(fig_box, use_container_width=True)
        
        # Heatmap
        st.subheader("ğŸ“Š Heatmap des Variables MÃ©tÃ©o")
        corr_matrix = df_noaa[['Temperature (Â°C)', 'Humidity (%)', 'Wind Speed (km/h)', 'Pressure (hPa)']].corr()
        fig_heat = px.imshow(corr_matrix, 
                            text_auto=True, 
                            aspect="auto",
                            title='Matrice de CorrÃ©lation des Variables MÃ©tÃ©o',
                            color_continuous_scale='RdBu')
        st.plotly_chart(fig_heat, use_container_width=True)
    
    # ========== TAB 2 : USGS ==========
    with tab2:
        st.subheader("DonnÃ©es Sismiques USGS")
        
        # DonnÃ©es USGS simulÃ©es
        df_usgs = pd.DataFrame({
            'Magnitude': np.random.uniform(2, 8, 50),
            'Depth (km)': np.random.uniform(1, 100, 50),
            'Region': np.random.choice(['California', 'Alaska', 'Hawaii', 'Nevada'], 50),
            'Latitude': np.random.uniform(30, 50, 50),
            'Longitude': np.random.uniform(-130, -60, 50)
        })
        
        col1, col2 = st.columns(2)
        
        with col1:
            # Histogramme des magnitudes
            fig_hist = px.histogram(df_usgs, 
                                   x='Magnitude', 
                                   nbins=20,
                                   title='Distribution des Magnitudes',
                                   color='Region',
                                   marginal="box")
            fig_hist.update_layout(height=450)
            st.plotly_chart(fig_hist, use_container_width=True)
        
        with col2:
            # Box plot par rÃ©gion
            fig_box = px.box(df_usgs, 
                            x='Region', 
                            y='Magnitude',
                            title='Magnitudes par RÃ©gion',
                            color='Region')
            fig_box.update_layout(height=450)
            st.plotly_chart(fig_box, use_container_width=True)
        
        # Carte 3D
        st.subheader("ğŸ—ºï¸ Visualisation 3D")
        fig_3d = px.scatter_3d(df_usgs,
                              x='Longitude',
                              y='Latitude', 
                              z='Depth (km)',
                              color='Magnitude',
                              size='Magnitude',
                              title='Localisation 3D des SÃ©ismes',
                              hover_name='Region')
        st.plotly_chart(fig_3d, use_container_width=True)
    
    # ========== TAB 3 : CORRÃ‰LATIONS ==========
    with tab3:
        st.subheader("CorrÃ©lations NOAA-USGS")
        
        # DonnÃ©es corrÃ©lÃ©es simulÃ©es
        dates = pd.date_range('2024-01-01', periods=100)
        df_corr = pd.DataFrame({
            'Date': dates,
            'Temperature': np.random.normal(20, 5, 100),
            'Earthquake_Frequency': np.random.poisson(3, 100),
            'Precipitation': np.random.exponential(5, 100),
            'Seismic_Energy': np.random.exponential(10, 100)
        })
        
        # Matrice de corrÃ©lation
        corr_matrix = df_corr[['Temperature', 'Earthquake_Frequency', 'Precipitation', 'Seismic_Energy']].corr()
        
        fig_corr = px.imshow(corr_matrix, 
                            text_auto=True, 
                            aspect="auto",
                            title='ğŸ“Š Matrice de CorrÃ©lation NOAA-USGS',
                            color_continuous_scale='RdBu_r',
                            labels=dict(color="CorrÃ©lation"))
        fig_corr.update_layout(height=500)
        st.plotly_chart(fig_corr, use_container_width=True)
        
        # Insights
        st.info("""
        ## ğŸ” Insights des CorrÃ©lations
        
        **ğŸ“ˆ TempÃ©rature â†” FrÃ©quence sismique:** 
        - CorrÃ©lation faible (r â‰ˆ 0.15)
        - Ã€ Ã©tudier plus en dÃ©tail
        
        **ğŸŒ§ï¸ PrÃ©cipitation â†” Ã‰nergie sismique:** 
        - CorrÃ©lation nÃ©gative modÃ©rÃ©e (r â‰ˆ -0.32)
        - Les pÃ©riodes de fortes pluies semblent coÃ¯ncider avec une activitÃ© sismique rÃ©duite
        
        **ğŸ” Patterns saisonniers dÃ©tectÃ©s:**
        - Augmentation des sÃ©ismes en Ã©tÃ©
        - CorrÃ©lation avec la sÃ©cheresse
        """)

# ============================================
# PAGE 3 : ALERTES
# ============================================

elif menu == "ğŸš¨ Alertes":
    
    st.header("ğŸš¨ SystÃ¨me d'Alertes Temps RÃ©el")
    
    # ========== SIMULATION D'ALERTE ==========
    col1, col2 = st.columns([3, 1])
    
    with col1:
        st.subheader("Simulation d'Alerte")
        
        if st.button("ğŸ”„ Simuler nouvelle alerte", type="primary", use_container_width=True):
            # Types d'alertes
            alert_types = [
                {"emoji": "ğŸŒ‹", "message": "SÃ©isme magnitude 6.2", "location": "California", "source": "USGS"},
                {"emoji": "ğŸŒªï¸", "message": "TempÃªte tropicale", "location": "Texas", "source": "NOAA"},
                {"emoji": "ğŸŒ¡ï¸", "message": "Vague de chaleur extrÃªme", "location": "Arizona", "source": "NOAA"},
                {"emoji": "ğŸŒ§ï¸", "message": "Inondations majeures", "location": "Floride", "source": "NOAA"}
            ]
            
            # Choix alÃ©atoire
            alert = np.random.choice(alert_types)
            
            # Affichage de l'alerte
            with st.chat_message("warning"):
                st.warning(f"{alert['emoji']} **ALERTE : {alert['message']}**")
                st.write(f"ğŸ“ **Localisation :** {alert['location']}")
                st.write(f"ğŸ“¡ **Source :** {alert['source']}")
                st.write(f"â° **Timestamp :** {datetime.now().strftime('%H:%M:%S')}")
            
            # Confetti pour l'effet visuel
            st.balloons()
    
    with col2:
        st.subheader("Statistiques")
        st.metric("Alertes actives", "2")
        st.metric("DerniÃ¨re alerte", "15 min")
        st.metric("Taux d'alertes", "3/jour")
    
    # ========== HISTORIQUE DES ALERTES ==========
    st.markdown("---")
    st.subheader("ğŸ“‹ Historique des Alertes")
    
    # DonnÃ©es d'historique
    alerts_data = pd.DataFrame({
        'Timestamp': pd.date_range('2024-01-14', periods=10, freq='H'),
        'Type': np.random.choice(['SÃ©isme', 'TempÃªte', 'Inondation', 'Chaleur'], 10),
        'Severity': np.random.choice(['Faible', 'ModÃ©rÃ©e', 'Ã‰levÃ©e', 'Critique'], 10),
        'Region': np.random.choice(['Californie', 'Alaska', 'Texas', 'Floride'], 10),
        'Status': np.random.choice(['Active', 'RÃ©solue', 'En cours'], 10)
    })
    
    # Style conditionnel
    def severity_color(val):
        if val == 'Critique':
            return 'background-color: #ffcccc'
        elif val == 'Ã‰levÃ©e':
            return 'background-color: #ffebcc'
        elif val == 'ModÃ©rÃ©e':
            return 'background-color: #ffffcc'
        else:
            return 'background-color: #ccffcc'
    
    st.dataframe(
        alerts_data.style.applymap(severity_color, subset=['Severity']),
        use_container_width=True,
        height=300
    )
    
    # Graphique des alertes par type
    st.subheader("ğŸ“Š RÃ©partition des Alertes")
    alert_counts = alerts_data['Type'].value_counts().reset_index()
    alert_counts.columns = ['Type', 'Count']
    
    fig_alerts = px.pie(alert_counts, 
                       values='Count', 
                       names='Type',
                       title='RÃ©partition des Alertes par Type',
                       hole=0.3)
    st.plotly_chart(fig_alerts, use_container_width=True)

# ============================================
# PAGE 4 : HDFS EXPLORER
# ============================================

elif menu == "ğŸ“ HDFS Explorer":
    
    st.header("ğŸ“ Explorateur HDFS")
    
    # Information de connexion
    st.info("""
    **Connexion HDFS active** âœ…
    - **Namenode:** namenode:9000
    - **Chemin racine:** /hadoop-climate-risk
    - **Mode:** Simulation (donnÃ©es fictives)
    """)
    
    # ========== STRUCTURE HDFS ==========
    st.subheader("ğŸ—ï¸ Structure du DataLake")
    
    # Structure simulÃ©e
    hdfs_structure = {
        "ğŸ“ /hadoop-climate-risk": {
            "ğŸ“ raw (DonnÃ©es brutes)": {
                "ğŸ“ noaa": [
                    "noaa_20240114.parquet (1.2 GB)",
                    "noaa_20240113.parquet (1.1 GB)",
                    "noaa_20240112.parquet (1.3 GB)"
                ],
                "ğŸ“ usgs": [
                    "earthquakes_2024.parquet (850 MB)",
                    "seismic_data.parquet (720 MB)",
                    "usgs_latest.json (150 MB)"
                ]
            },
            "ğŸ“ silver (NettoyÃ©es)": {
                "ğŸ“ cleaned": [
                    "noaa_cleaned.parquet (980 MB)",
                    "usgs_cleaned.parquet (680 MB)"
                ],
                "ğŸ“ normalized": [
                    "data_normalized.parquet (1.5 GB)"
                ]
            },
            "ğŸ“ gold (AgrÃ©gÃ©es)": {
                "ğŸ“ aggregates": [
                    "daily_aggregates.parquet (320 MB)",
                    "monthly_trends.parquet (180 MB)",
                    "weekly_report.parquet (95 MB)"
                ],
                "ğŸ“ reports": [
                    "climate_report.json (45 MB)",
                    "seismic_analysis.json (38 MB)",
                    "correlation_study.json (52 MB)"
                ]
            },
            "ğŸ“ alerts (Streaming)": {
                "ğŸ“ kafka": [
                    "climate-alerts.parquet (210 MB)",
                    "alerts_stream.parquet (185 MB)"
                ],
                "ğŸ“ processed": [
                    "alerts_processed.parquet (120 MB)",
                    "anomalies_detected.parquet (95 MB)"
                ]
            }
        }
    }
    
    # Fonction rÃ©cursive pour afficher l'arborescence
    def display_tree(structure, level=0):
        for key, value in structure.items():
            if isinstance(value, dict):
                with st.expander(f"{'  ' * level}{key}"):
                    display_tree(value, level + 1)
            else:
                for item in value:
                    col1, col2 = st.columns([4, 1])
                    with col1:
                        st.write(f"{'  ' * (level + 1)}ğŸ“„ {item}")
                    with col2:
                        if st.button("AperÃ§u", key=f"view_{item}"):
                            st.info(f"AperÃ§u de {item} - DonnÃ©es simulÃ©es")
    
    display_tree(hdfs_structure)
    
    # ========== STATISTIQUES HDFS ==========
    st.markdown("---")
    st.subheader("ğŸ“Š Statistiques HDFS")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Fichiers totaux", "42", "+3")
    
    with col2:
        st.metric("Taille totale", "2.4 GB", "+0.3 GB")
    
    with col3:
        st.metric("DerniÃ¨re mise Ã  jour", "15:30")
    
    with col4:
        st.metric("Espace utilisÃ©", "78%")
    
    # ========== APERÃ‡U DES DONNÃ‰ES ==========
    st.subheader("ğŸ‘ï¸ AperÃ§u des DonnÃ©es")
    
    file_to_preview = st.selectbox(
        "SÃ©lectionner un fichier Ã  prÃ©visualiser",
        [
            "noaa_20240114.parquet",
            "earthquakes_2024.parquet", 
            "daily_aggregates.parquet",
            "climate_report.json"
        ]
    )
    
    if st.button("ğŸ“– Afficher l'aperÃ§u"):
        # DonnÃ©es simulÃ©es selon le type de fichier
        if "noaa" in file_to_preview:
            sample_data = pd.DataFrame({
                'date': pd.date_range('2024-01-01', periods=10),
                'station_id': ['NYC001', 'LAX002', 'CHI003', 'MIA004', 'SEA005'] * 2,
                'temperature': np.random.normal(15, 5, 10),
                'precipitation': np.random.exponential(2, 10),
                'humidity': np.random.uniform(30, 90, 10)
            })
        elif "earthquake" in file_to_preview:
            sample_data = pd.DataFrame({
                'timestamp': pd.date_range('2024-01-01', periods=10, freq='H'),
                'magnitude': np.random.uniform(2, 8, 10),
                'latitude': np.random.uniform(30, 50, 10),
                'longitude': np.random.uniform(-130, -60, 10),
                'depth_km': np.random.uniform(1, 100, 10)
            })
        else:
            sample_data = pd.DataFrame({
                'metric': ['TempÃ©rature moyenne', 'PrÃ©cipitation totale', 'SÃ©ismes count'],
                'value': [15.2, 42.5, 24],
                'unit': ['Â°C', 'mm', 'count']
            })
        
        st.dataframe(sample_data, use_container_width=True)
        st.info(f"ğŸ“„ Fichier: {file_to_preview} | ğŸ“Š Lignes: {len(sample_data)} | ğŸ“ Colonnes: {len(sample_data.columns)}")

# ============================================
# PAGE 5 : ADMINISTRATION
# ============================================

elif menu == "âš™ï¸ Administration":
    
    st.header("âš™ï¸ Administration du DataLake")
    
    # Onglets
    tab1, tab2, tab3 = st.tabs(["ğŸ“¥ Ingestion", "ğŸ”§ Traitement", "ğŸ“¤ Export"])
    
    # ========== TAB 1 : INGESTION ==========
    with tab1:
        st.subheader("Ingestion des DonnÃ©es")
        
        col1, col2 = st.columns(2)
        
        # Bouton NOAA
        with col1:
            if st.button("ğŸš€ Lancer ingestion NOAA", 
                        use_container_width=True,
                        help="Collecte des donnÃ©es mÃ©tÃ©o depuis l'API NOAA"):
                with st.spinner("Connexion Ã  l'API NOAA..."):
                    # Barre de progression
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    
                    for i in range(100):
                        time.sleep(0.02)
                        progress_bar.progress(i + 1)
                        
                        # Mise Ã  jour du statut
                        if i < 30:
                            status_text.text("ğŸ”Œ Connexion Ã  l'API...")
                        elif i < 60:
                            status_text.text("ğŸ“¥ TÃ©lÃ©chargement des donnÃ©es...")
                        elif i < 90:
                            status_text.text("ğŸ’¾ Ã‰criture vers HDFS...")
                        else:
                            status_text.text("âœ… Finalisation...")
                    
                    # RÃ©sultats
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    st.success("âœ… 1,250 enregistrements NOAA ingÃ©rÃ©s")
                    st.info(f"""
                    **Chemin HDFS:** `/hadoop-climate-risk/raw/noaa/noaa_{timestamp}.parquet`
                    
                    **DÃ©tails:**
                    - Format: Parquet (compressÃ©)
                    - Taille: ~45 MB
                    - PÃ©riode: Derniers 30 jours
                    - Stations: 15 stations mÃ©tÃ©o
                    """)
                    
                    # AperÃ§u des donnÃ©es
                    with st.expander("ğŸ“‹ AperÃ§u des donnÃ©es ingÃ©rÃ©es"):
                        sample_df = pd.DataFrame({
                            'date': pd.date_range('2024-01-01', periods=5),
                            'station': ['NYC001', 'LAX002', 'CHI003', 'MIA004', 'SEA005'],
                            'temperature': [15.2, 18.5, 12.3, 24.1, 10.8],
                            'humidity': [65, 42, 78, 85, 55],
                            'wind_speed': [12.3, 8.7, 15.2, 5.4, 20.1]
                        })
                        st.dataframe(sample_df)
        
        # Bouton USGS
        with col2:
            if st.button("ğŸš€ Lancer ingestion USGS", 
                        use_container_width=True,
                        help="Collecte des donnÃ©es sismiques depuis l'API USGS"):
                with st.spinner("Connexion Ã  l'API USGS..."):
                    # Barre de progression
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    
                    for i in range(100):
                        time.sleep(0.02)
                        progress_bar.progress(i + 1)
                        
                        # Mise Ã  jour du statut
                        if i < 30:
                            status_text.text("ğŸ”Œ Connexion Ã  l'API...")
                        elif i < 60:
                            status_text.text("ğŸ“¥ TÃ©lÃ©chargement des sÃ©ismes...")
                        elif i < 90:
                            status_text.text("ğŸ’¾ Ã‰criture vers HDFS...")
                        else:
                            status_text.text("âœ… Finalisation...")
                    
                    # RÃ©sultats
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    st.success("âœ… 850 sÃ©ismes USGS collectÃ©s")
                    st.info(f"""
                    **Chemin HDFS:** `/hadoop-climate-risk/raw/usgs/earthquakes_{timestamp}.parquet`
                    
                    **DÃ©tails:**
                    - Format: Parquet (compressÃ©)
                    - Taille: ~38 MB
                    - PÃ©riode: Derniers 7 jours
                    - Magnitude min: 2.5
                    """)
                    
                    # AperÃ§u des donnÃ©es
                    with st.expander("ğŸ“‹ AperÃ§u des donnÃ©es ingÃ©rÃ©es"):
                        sample_df = pd.DataFrame({
                            'timestamp': pd.date_range('2024-01-01', periods=5, freq='H'),
                            'magnitude': [4.5, 3.2, 5.1, 2.8, 4.9],
                            'location': ['California', 'Alaska', 'Hawaii', 'Nevada', 'Texas'],
                            'depth_km': [10.2, 15.5, 8.7, 22.1, 12.4],
                            'latitude': [34.05, 36.17, 37.77, 40.71, 47.61],
                            'longitude': [-118.25, -120.72, -122.42, -74.01, -122.33]
                        })
                        st.dataframe(sample_df)
        
        # Upload manuel
        st.markdown("---")
        st.subheader("ğŸ“¤ Upload Manuel")
        
        uploaded_file = st.file_uploader(
            "Choisir un fichier Ã  uploader vers HDFS",
            type=['csv', 'json', 'parquet', 'txt'],
            help="Formats supportÃ©s: CSV, JSON, Parquet"
        )
        
        if uploaded_file is not None:
            # Afficher les informations du fichier
            file_details = {
                "Nom": uploaded_file.name,
                "Type": uploaded_file.type,
                "Taille": f"{uploaded_file.size / 1024 / 1024:.2f} MB"
            }
            
            col1, col2 = st.columns(2)
            with col1:
                st.success(f"âœ… {uploaded_file.name} prÃªt Ã  Ãªtre uploadÃ©")
                for key, value in file_details.items():
                    st.write(f"**{key}:** {value}")
            
            with col2:
                destination = st.selectbox(
                    "Destination HDFS",
                    [
                        "/hadoop-climate-risk/raw/noaa/",
                        "/hadoop-climate-risk/raw/usgs/",
                        "/hadoop-climate-risk/alerts/",
                        "/hadoop-climate-risk/temp/"
                    ]
                )
                
                if st.button("ğŸ“¤ Upload vers HDFS", type="primary"):
                    with st.spinner(f"Upload vers {destination}..."):
                        time.sleep(2)
                        st.success(f"âœ… Fichier uploadÃ© vers {destination}{uploaded_file.name}")
                        
                        # AperÃ§u si c'est un CSV
                        if uploaded_file.name.endswith('.csv'):
                            df_upload = pd.read_csv(uploaded_file)
                            st.dataframe(df_upload.head(10), use_container_width=True)
    
    # ========== TAB 2 : TRAITEMENT ==========
    with tab2:
        st.subheader("Traitement Spark")
        
        # Liste des jobs
        jobs = {
            "ğŸ§¹ Nettoyage ETL": {
                "desc": "Nettoyage des donnÃ©es brutes (valeurs manquantes, outliers)",
                "time": "3-5 min",
                "input": "/hadoop-climate-risk/raw/",
                "output": "/hadoop-climate-risk/silver/"
            },
            "ğŸ“Š AgrÃ©gation quotidienne": {
                "desc": "Calcul des statistiques journaliÃ¨res",
                "time": "2-3 min",
                "input": "/hadoop-climate-risk/silver/",
                "output": "/hadoop-climate-risk/gold/aggregates/"
            },
            "ğŸš¨ DÃ©tection d'anomalies": {
                "desc": "Identification des valeurs aberrantes",
                "time": "4-6 min",
                "input": "/hadoop-climate-risk/silver/",
                "output": "/hadoop-climate-risk/gold/anomalies/"
            },
            "ğŸ“ˆ Calcul des tendances": {
                "desc": "Analyse des tendances long terme",
                "time": "5-7 min",
                "input": "/hadoop-climate-risk/gold/",
                "output": "/hadoop-climate-risk/gold/trends/"
            }
        }
        
        # SÃ©lection du job
        selected_job = st.selectbox(
            "SÃ©lectionner un job Spark Ã  exÃ©cuter",
            list(jobs.keys()),
            format_func=lambda x: f"{x} - {jobs[x]['desc'][:50]}..."
        )
        
        # Afficher les dÃ©tails du job
        if selected_job:
            job_info = jobs[selected_job]
            st.info(f"""
            **Description:** {job_info['desc']}
            
            **Estimation temps:** {job_info['time']}
            **Input:** {job_info['input']}
            **Output:** {job_info['output']}
            """)
        
        # Bouton d'exÃ©cution
        if st.button(f"âš¡ ExÃ©cuter {selected_job}", type="primary", use_container_width=True):
            with st.spinner(f"ExÃ©cution du job Spark: {selected_job}..."):
                # Barre de progression
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                for percent in range(100):
                    time.sleep(0.03)
                    progress_bar.progress(percent + 1)
                    
                    # Messages de statut
                    if percent < 20:
                        status_text.text("ğŸš€ Initialisation du job Spark...")
                    elif percent < 40:
                        status_text.text("ğŸ“– Lecture des donnÃ©es depuis HDFS...")
                    elif percent < 60:
                        status_text.text("âš™ï¸ Traitement des donnÃ©es...")
                    elif percent < 80:
                        status_text.text("ğŸ’¾ Ã‰criture des rÃ©sultats...")
                    else:
                        status_text.text("âœ… Finalisation...")
                
                # RÃ©sultats
                job_id = f"spark-{int(time.time())}"
                
                st.success(f"âœ… Job {selected_job} terminÃ© avec succÃ¨s")
                
                # DÃ©tails d'exÃ©cution
                with st.expander("ğŸ“‹ DÃ©tails d'exÃ©cution", expanded=True):
                    st.code(f"""
                    Job ID: {job_id}
                    Status: SUCCEEDED
                    Duration: 2m 45s
                    Start Time: {datetime.now().strftime('%H:%M:%S')}
                    End Time: {(datetime.now() + timedelta(minutes=2, seconds=45)).strftime('%H:%M:%S')}
                    
                    Configuration Spark:
                    - Application: DataLake_ETL
                    - Master: spark://spark-master:7077
                    - Driver Memory: 2g
                    - Executor Cores: 2
                    - Partitions: 10
                    
                    MÃ©triques:
                    - Records processed: 15,230
                    - Data read: 1.8 GB
                    - Data written: 450 MB
                    - Shuffle spill: 0 bytes
                    
                    Output:
                    - HDFS Path: {job_info['output']}
                    - Files: 12 part-*.parquet files
                    - Format: Parquet (snappy compression)
                    """)
                
                # Ã‰chantillon des rÃ©sultats pour certains jobs
                if "AgrÃ©gation" in selected_job:
                    st.subheader("ğŸ“Š Ã‰chantillon des RÃ©sultats")
                    
                    # DonnÃ©es simulÃ©es
                    if "quotidienne" in selected_job:
                        sample_results = pd.DataFrame({
                            'date': pd.date_range('2024-01-01', periods=7),
                            'avg_temperature': np.random.normal(15, 2, 7),
                            'max_temperature': np.random.normal(20, 3, 7),
                            'min_temperature': np.random.normal(10, 2, 7),
                            'total_precipitation': np.random.exponential(5, 7),
                            'earthquake_count': np.random.poisson(3, 7)
                        })
                    else:
                        sample_results = pd.DataFrame({
                            'metric': ['TempÃ©rature moyenne', 'PrÃ©cipitation annuelle', 'SÃ©ismes majeurs'],
                            'value': [15.2, 1250.5, 24],
                            'trend': ['â†‘ +0.5Â°C', 'â†“ -10%', 'â†’ stable'],
                            'period': ['2024', '2023-2024', 'Dernier mois']
                        })
                    
                    st.dataframe(sample_results, use_container_width=True)
    
    # ========== TAB 3 : EXPORT ==========
    with tab3:
        st.subheader("Export et Rapports")
        
        # Types de rapports
        report_types = {
            "ğŸ“„ Rapport climatique complet": {
                "desc": "Analyse dÃ©taillÃ©e des tendances mÃ©tÃ©orologiques",
                "size": "45 pages",
                "content": "Ce rapport analyse les tendances climatiques sur la derniÃ¨re dÃ©cennie..."
            },
            "ğŸ“„ Analyse sismique rÃ©gionale": {
                "desc": "ActivitÃ© sismique par rÃ©gion avec visualisations",
                "size": "32 pages",
                "content": "Analyse de l'activitÃ© sismique dans les rÃ©gions Ã  risque..."
            },
            "ğŸ“„ Ã‰tude de corrÃ©lation NOAA-USGS": {
                "desc": "CorrÃ©lations entre donnÃ©es mÃ©tÃ©o et sismiques",
                "size": "28 pages",
                "content": "Ã‰tude statistique des corrÃ©lations entre variables climatiques et sismiques..."
            },
            "ğŸ“Š Dashboard interactif (PDF)": {
                "desc": "Export PDF du dashboard actuel",
                "size": "15 pages",
                "content": "Snapshot interactif du dashboard DataLake avec toutes les visualisations..."
            },
            "ğŸ’¾ Dataset complet (CSV)": {
                "desc": "Export des donnÃ©es nettoyÃ©es au format CSV",
                "size": "~850 MB",
                "content": "Dataset complet prÃªt pour analyse externe..."
            }
        }
        
        # SÃ©lection du rapport
        selected_report = st.selectbox(
            "SÃ©lectionner un rapport Ã  gÃ©nÃ©rer",
            list(report_types.keys())
        )
        
        if selected_report:
            report_info = report_types[selected_report]
            
            st.info(f"""
            **Description:** {report_info['desc']}
            **Taille estimÃ©e:** {report_info['size']}
            **GÃ©nÃ©ration:** ~30 secondes
            """)
            
            # Bouton de gÃ©nÃ©ration
            if st.button(f"ğŸ”„ GÃ©nÃ©rer {selected_report}", type="primary", use_container_width=True):
                with st.spinner(f"GÃ©nÃ©ration du {selected_report}..."):
                    # Simulation de gÃ©nÃ©ration
                    progress_bar = st.progress(0)
                    
                    for i in range(100):
                        time.sleep(0.02)
                        progress_bar.progress(i + 1)
                    
                    st.success(f"âœ… {selected_report} gÃ©nÃ©rÃ© avec succÃ¨s")
                    
                    # Bouton de tÃ©lÃ©chargement
                    st.download_button(
                        label=f"ğŸ“¥ TÃ©lÃ©charger {selected_report}",
                        data=report_info['content'],
                        file_name=f"{selected_report.lower().replace('ğŸ“„ ', '').replace('ğŸ“Š ', '').replace('ğŸ’¾ ', '').replace(' ', '_')}.txt",
                        mime="text/plain",
                        help="Fichier de dÃ©monstration - en production, ce serait un PDF ou CSV"
                    )
                    
                    # Information supplÃ©mentaire
                    st.info(f"""
                    **Fichier gÃ©nÃ©rÃ©:** `{selected_report.replace('ğŸ“„ ', '').replace('ğŸ“Š ', '').replace('ğŸ’¾ ', '').replace(' ', '_')}.pdf`
                    **Chemin HDFS:** `/hadoop-climate-risk/gold/reports/{datetime.now().strftime('%Y%m%d')}/`
                    **Timestamp:** {datetime.now().strftime('%H:%M:%S')}
                    """)

# ============================================
# PAGE 6 : ARCHITECTURE
# ============================================

elif menu == "ğŸ—ï¸ Architecture":
    
    st.header("ğŸ—ï¸ Architecture du DataLake")
    
    # ========== DIAGRAMME D'ARCHITECTURE ==========
    st.markdown("""
    ### ğŸ“Š Architecture Big Data ComplÃ¨te
    
    ```ascii
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    COUCHE PRÃ‰SENTATION (Streamlit)                  â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚  â”‚                 Streamlit Dashboard v1.0                     â”‚  â”‚
    â”‚  â”‚  â€¢ Visualisations Plotly interactives                        â”‚  â”‚
    â”‚  â”‚  â€¢ Interface utilisateur multi-pages                         â”‚  â”‚
    â”‚  â”‚  â€¢ ContrÃ´les d'administration                                â”‚  â”‚
    â”‚  â”‚  â€¢ Alertes temps rÃ©el                                        â”‚  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    â”‚                                 â”‚ HTTP/WebSocket                    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚          COUCHE TRAITEMENT (Spark + Python)                        â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
    â”‚  â”‚   Spark     â”‚  â”‚   Kafka     â”‚  â”‚   Python    â”‚                â”‚
    â”‚  â”‚  Cluster    â”‚  â”‚  Streaming  â”‚  â”‚  Ingestion  â”‚                â”‚
    â”‚  â”‚  â€¢ ETL Jobs â”‚  â”‚  â€¢ Topics   â”‚  â”‚  â€¢ APIs     â”‚                â”‚
    â”‚  â”‚  â€¢ ML/Stats â”‚  â”‚  â€¢ Alerts   â”‚  â”‚  â€¢ Batch    â”‚                â”‚
    â”‚  â”‚  â€¢ Analyticsâ”‚  â”‚  â€¢ Logs     â”‚  â”‚  â€¢ Scripts  â”‚                â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                â”‚
    â”‚         â”‚                â”‚                â”‚                        â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                    COUCHE STOCKAGE (HDFS)                          â”‚
    â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
    â”‚         â”‚              HDFS DataLake                   â”‚           â”‚
    â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚           â”‚
    â”‚         â”‚  â”‚  RAW    â”‚  SILVER â”‚   GOLD  â”‚ ALERTS  â”‚  â”‚           â”‚
    â”‚         â”‚  â”‚  Layer  â”‚  Layer  â”‚  Layer  â”‚  Layer  â”‚  â”‚           â”‚
    â”‚         â”‚  â”‚  â€¢ NOAA â”‚  â€¢ ETL  â”‚ â€¢ Aggr. â”‚ â€¢ Kafka â”‚  â”‚           â”‚
    â”‚         â”‚  â”‚  â€¢ USGS â”‚  â€¢ Cleanâ”‚ â€¢ Stats â”‚ â€¢ Streamâ”‚  â”‚           â”‚
    â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚           â”‚
    â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                    COUCHE SOURCES (APIs)                           â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
    â”‚  â”‚      NOAA API     â”‚            â”‚      USGS API     â”‚           â”‚
    â”‚  â”‚  â€¢ weather.gov    â”‚            â”‚  earthquake.usgs  â”‚           â”‚
    â”‚  â”‚  â€¢ data.noaa.gov  â”‚            â”‚  â€¢ Realtime       â”‚           â”‚
    â”‚  â”‚  â€¢ CSV/JSON       â”‚            â”‚  â€¢ Historical     â”‚           â”‚
    â”‚  â”‚  â€¢ Realtime       â”‚            â”‚  â€¢ GeoJSON        â”‚           â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    ```
    """)
    
    # ========== STACK TECHNOLOGIQUE ==========
    st.markdown("### ğŸ› ï¸ Stack Technologique")
    
    tech_stack = pd.DataFrame({
        "Couche": ["Stockage", "Traitement", "Streaming", "Visualisation", "Orchestration", "Sources"],
        "Technologies": [
            "HDFS (Hadoop Distributed File System)",
            "Apache Spark, Python (Pandas, PySpark)",
            "Apache Kafka, Spark Streaming",
            "Streamlit, Plotly, Altair",
            "Docker, Docker Compose, Kubernetes",
            "NOAA API, USGS API, OpenData"
        ],
        "RÃ´le": [
            "Stockage distribuÃ© des donnÃ©es brutes et transformÃ©es",
            "ETL, analyse, machine learning, agrÃ©gations",
            "Traitement temps rÃ©el, alertes, monitoring",
            "Dashboard interactif, visualisations, rapports",
            "Conteneurisation, dÃ©ploiement, scaling",
            "Sources de donnÃ©es externes en temps rÃ©el"
        ]
    })
    
    st.dataframe(tech_stack, use_container_width=True, hide_index=True)
    
    # ========== FLUX DE DONNÃ‰ES ==========
    st.markdown("### ğŸ“ˆ Flux de DonnÃ©es")
    
    st.info("""
    **1. ğŸ“¥ Ingestion (Batch + Streaming)**
    ```
    Sources externes â†’ Python Scripts â†’ HDFS (Raw) + Kafka
    ```
    
    **2. âš™ï¸ Transformation (ETL)**
    ```
    HDFS (Raw) â†’ Spark Jobs â†’ HDFS (Silver)
    â€¢ Nettoyage des donnÃ©es
    â€¢ Normalisation
    â€¢ Enrichissement
    ```
    
    **3. ğŸ“Š Analyse & AgrÃ©gation**
    ```
    HDFS (Silver) â†’ Spark Analytics â†’ HDFS (Gold)
    â€¢ AgrÃ©gations temporelles
    â€¢ Calculs statistiques
    â€¢ Machine Learning
    â€¢ DÃ©tection d'anomalies
    ```
    
    **4. ğŸ¯ Visualisation & Insights**
    ```
    HDFS (Gold) â†’ Streamlit â†’ Dashboard
    â€¢ Visualisations interactives
    â€¢ Rapports automatiques
    â€¢ Alertes temps rÃ©el
    â€¢ Export de donnÃ©es
    ```
    
    **5. ğŸ”„ Streaming & Monitoring**
    ```
    Kafka Topics â†’ Spark Streaming â†’ Alerts â†’ HDFS/Streamlit
    â€¢ Monitoring continu
    â€¢ Alertes en temps rÃ©el
    â€¢ Logs et mÃ©triques
    ```
    """)
    
    # ========== AVANTAGES ==========
    st.markdown("### ğŸ¯ Avantages de l'Architecture")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("""
        **ğŸš€ ScalabilitÃ©**
        - HDFS: Stockage distribuÃ© illimitÃ©
        - Spark: Traitement parallÃ¨le
        - Docker: DÃ©ploiement flexible
        """)
    
    with col2:
        st.markdown("""
        **ğŸ”„ Temps RÃ©el**
        - Kafka: Streaming de donnÃ©es
        - Alertes instantanÃ©es
        - Monitoring continu
        """)
    
    with col3:
        st.markdown("""
        **ğŸ”§ Maintenance**
        - Architecture modulaire
        - Code versionnÃ© (Git)
        - Documentation complÃ¨te
        """)
    
    # ========== MÃ‰TRIQUES DE PERFORMANCE ==========
    st.markdown("### ğŸ“Š MÃ©triques de Performance")
    
    metrics = pd.DataFrame({
        "MÃ©trique": ["Latence ingestion", "Temps traitement", "DisponibilitÃ©", "Volume donnÃ©es", "CoÃ»t stockage"],
        "Valeur": ["< 5 min", "2-5 min/job", "99.9%", "~2.4 GB", "~$15/mois"],
        "Objectif": ["Temps rÃ©el", "RapiditÃ©", "Haute dispo", "Scalable", "Ã‰conomique"]
    })
    
    st.dataframe(metrics, use_container_width=True, hide_index=True)

# ============================================
# FOOTER
# ============================================

st.sidebar.markdown("---")
st.sidebar.markdown("**DataLake Climat & Risques Naturels**")
st.sidebar.markdown("*Projet Big Data - Architecture*")

# Informations de version
with st.sidebar.expander("â„¹ï¸ Informations"):
    st.write(f"**Version:** 1.0.0")
    st.write(f"**DerniÃ¨re mise Ã  jour:** {datetime.now().strftime('%d/%m/%Y')}")
    st.write(f"**Environnement:** Production")
    st.write("**Ã‰quipe:** Data Engineering Team")
    st.write("**Contact:** datalake@climate-risks.com")

# Note de bas de page
st.sidebar.caption("""
âš ï¸ **Note:** Cette application est une dÃ©monstration.
Les donnÃ©es sont simulÃ©es pour illustrer les capacitÃ©s du DataLake.
""")