import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
import time
from utils.hdfs_client import HDFSClient
import json
import random

st.set_page_config(page_title="Alertes Temps R√©el", layout="wide")
st.title("üö® Alertes Temps R√©el")
st.markdown("### Monitoring des √©v√©nements climatiques et sismiques en direct")

# Initialisation clients
hdfs_client = HDFSClient()

# Configuration
if "auto_refresh" not in st.session_state:
    st.session_state.auto_refresh = False
if "alert_history" not in st.session_state:
    st.session_state.alert_history = []

# Sidebar pour les param√®tres
with st.sidebar:
    st.header("‚öôÔ∏è Param√®tres")
    
    # Type d'alertes √† afficher
    alert_types = st.multiselect(
        "Types d'alertes",
        ["S√©ismes", "Temp√™tes", "Inondations", "Vagues de chaleur", "Incendies"],
        default=["S√©ismes", "Temp√™tes"]
    )
    
    # Niveau de s√©v√©rit√©
    severity_level = st.slider(
        "Niveau de s√©v√©rit√© minimum",
        min_value=1, max_value=5, value=2,
        help="1=Faible, 3=Mod√©r√©, 5=Extr√™me"
    )
    
    # R√©gions
    regions = st.multiselect(
        "R√©gions",
        ["Am√©rique du Nord", "Europe", "Asie", "Am√©rique du Sud", "Afrique", "Oc√©anie"],
        default=["Am√©rique du Nord", "Europe"]
    )
    
    # Auto-refresh
    auto_refresh = st.checkbox("üîÑ Auto-refresh", value=st.session_state.auto_refresh)
    refresh_interval = st.slider("Intervalle (secondes)", 5, 60, 10, disabled=not auto_refresh)
    
    if auto_refresh != st.session_state.auto_refresh:
        st.session_state.auto_refresh = auto_refresh
        st.rerun()

# Fonctions de simulation
def simulate_real_time_alerts():
    """Simule la r√©ception d'alertes temps r√©el"""
    alert_types_sim = ["S√©isme", "Temp√™te", "Inondation", "Vague de chaleur", "Incendie"]
    regions_sim = ["Californie", "Alaska", "Floride", "Texas", "New York", "Qu√©bec", "Paris", "Tokyo"]
    
    alert = {
        "id": f"alert_{int(time.time())}_{random.randint(1000, 9999)}",
        "type": random.choice(alert_types),
        "severity": random.randint(1, 5),
        "region": random.choice(regions_sim),
        "latitude": round(random.uniform(30, 50), 4),
        "longitude": round(random.uniform(-130, -60), 4),
        "magnitude": round(random.uniform(2.0, 8.0), 1) if random.random() > 0.5 else None,
        "wind_speed": round(random.uniform(50, 200), 1) if random.random() > 0.5 else None,
        "temperature": round(random.uniform(35, 50), 1) if random.random() > 0.5 else None,
        "description": f"√âv√©nement {random.choice(['majeur', 'mod√©r√©', 'mineur'])} d√©tect√©",
        "timestamp": datetime.now().isoformat(),
        "source": random.choice(["USGS", "NOAA", "NASA", "M√©t√©o France"])
    }
    
    # Ajouter √† l'historique
    st.session_state.alert_history.append(alert)
    
    # Garder seulement les 100 derni√®res alertes
    if len(st.session_state.alert_history) > 100:
        st.session_state.alert_history = st.session_state.alert_history[-100:]
    
    return alert

def load_alerts_from_hdfs():
    """Charge les alertes depuis HDFS"""
    alerts = []
    
    if hdfs_client.connected:
        try:
            # Lire les fichiers d'alertes
            alert_files = hdfs_client.list_files("/hadoop-climate-risk/alerts/")
            
            for file in alert_files[-5:]:  # 5 fichiers les plus r√©cents
                if file.endswith(".parquet"):
                    df = hdfs_client.read_parquet_head(file, 20)
                    if not df.empty and "Error" not in df.columns:
                        for _, row in df.iterrows():
                            alert = {
                                "type": row.get("alert_type", "Inconnu"),
                                "severity": row.get("severity", 1),
                                "region": row.get("region", "Inconnu"),
                                "latitude": row.get("latitude", 0.0),
                                "longitude": row.get("longitude", 0.0),
                                "timestamp": row.get("timestamp", datetime.now().isoformat()),
                                "description": row.get("description", "Alerte non sp√©cifi√©e"),
                                "source": "HDFS"
                            }
                            alerts.append(alert)
        except:
            pass
    
    return alerts

# Layout principal
col1, col2 = st.columns([2, 1])

with col1:
    # Carte des alertes en temps r√©el
    st.subheader("üìç Carte des Alertes Actives")
    
    # Pr√©parer les donn√©es pour la carte
    all_alerts = load_alerts_from_hdfs()
    
    # Ajouter des alertes simul√©es
    if st.button("üîÑ Simuler nouvelle alerte"):
        new_alert = simulate_real_time_alerts()
        all_alerts.append(new_alert)
        st.success(f"‚úÖ Nouvelle alerte simul√©e: {new_alert['type']} - {new_alert['region']}")
    
    if all_alerts:
        # Filtrer par s√©v√©rit√©
        filtered_alerts = [a for a in all_alerts if a.get("severity", 1) >= severity_level]
        
        # Filtrer par type
        if alert_types:
            filtered_alerts = [a for a in filtered_alerts 
                             if any(t in a.get("type", "") for t in alert_types)]
        
        # Filtrer par r√©gion
        if regions:
            filtered_alerts = [a for a in filtered_alerts 
                             if any(r in a.get("region", "") for r in regions)]
        
        if filtered_alerts:
            # Cr√©er DataFrame pour Plotly
            df_alerts = pd.DataFrame(filtered_alerts)
            
            # Personnaliser la taille des marqueurs par s√©v√©rit√©
            df_alerts["size"] = df_alerts["severity"] * 10
            
            # Personnaliser les couleurs par type
            color_discrete_map = {
                "S√©isme": "red",
                "Temp√™te": "blue", 
                "Inondation": "cyan",
                "Vague de chaleur": "orange",
                "Incendie": "darkred"
            }
            
            fig = px.scatter_mapbox(
                df_alerts,
                lat="latitude",
                lon="longitude",
                color="type",
                size="size",
                hover_name="description",
                hover_data=["region", "severity", "timestamp", "source"],
                zoom=2,
                height=500,
                color_discrete_map=color_discrete_map
            )
            
            fig.update_layout(
                mapbox_style="open-street-map",
                margin={"r":0, "t":0, "l":0, "b":0}
            )
            
            st.plotly_chart(fig, use_container_width=True)
            
            # Statistiques
            col_stats1, col_stats2, col_stats3 = st.columns(3)
            with col_stats1:
                st.metric("Alertes actives", len(filtered_alerts))
            with col_stats2:
                avg_severity = df_alerts["severity"].mean() if not df_alerts.empty else 0
                st.metric("S√©v√©rit√© moyenne", f"{avg_severity:.1f}/5")
            with col_stats3:
                if not df_alerts.empty:
                    most_common = df_alerts["type"].mode().iloc[0] if not df_alerts["type"].mode().empty else "Aucun"
                    st.metric("Type dominant", most_common)
        else:
            st.info("‚ÑπÔ∏è Aucune alerte ne correspond aux filtres actuels")
    else:
        st.warning("‚ö†Ô∏è Aucune alerte disponible. Simulez une alerte ou v√©rifiez HDFS.")

with col2:
    # Liste des derni√®res alertes
    st.subheader("üìã Derni√®res Alertes")
    
    # Trier par timestamp
    if st.session_state.alert_history:
        recent_alerts = sorted(
            st.session_state.alert_history,
            key=lambda x: x.get("timestamp", ""),
            reverse=True
        )[:10]  # 10 plus r√©centes
        
        for alert in recent_alerts:
            # D√©terminer l'ic√¥ne et la couleur par type
            alert_type = alert.get("type", "Inconnu")
            severity = alert.get("severity", 1)
            
            icons = {
                "S√©isme": "üåã",
                "Temp√™te": "üå™Ô∏è",
                "Inondation": "üåä",
                "Vague de chaleur": "üî•",
                "Incendie": "üöí"
            }
            
            icon = icons.get(alert_type, "‚ö†Ô∏è")
            
            # Couleur par s√©v√©rit√©
            severity_colors = {
                1: "üü¢", 2: "üü°", 3: "üü†", 4: "üî¥", 5: "üíÄ"
            }
            
            severity_icon = severity_colors.get(severity, "‚ö™")
            
            # Afficher l'alerte
            with st.expander(f"{icon} {severity_icon} {alert_type} - {alert.get('region', 'Inconnu')}"):
                st.write(f"**Description:** {alert.get('description', 'N/A')}")
                st.write(f"**S√©v√©rit√©:** {severity}/5")
                st.write(f"**Source:** {alert.get('source', 'Inconnu')}")
                
                if alert.get("magnitude"):
                    st.write(f"**Magnitude:** {alert['magnitude']}")
                if alert.get("wind_speed"):
                    st.write(f"**Vitesse vent:** {alert['wind_speed']} km/h")
                if alert.get("temperature"):
                    st.write(f"**Temp√©rature:** {alert['temperature']}¬∞C")
                
                st.caption(f"‚è∞ {alert.get('timestamp', '')}")
    else:
        st.info("Aucune alerte r√©cente. Cliquez sur 'Simuler nouvelle alerte'")

# Section analyse temporelle
st.markdown("---")
st.subheader("üìà Analyse Temporelle des Alertes")

if st.session_state.alert_history:
    df_history = pd.DataFrame(st.session_state.alert_history)
    
    # Convertir les timestamps
    df_history["datetime"] = pd.to_datetime(df_history["timestamp"])
    df_history["hour"] = df_history["datetime"].dt.hour
    df_history["date"] = df_history["datetime"].dt.date
    
    # Graphique 1: Alertes par heure
    col_hist1, col_hist2 = st.columns(2)
    
    with col_hist1:
        hourly_counts = df_history.groupby("hour").size().reset_index(name="count")
        fig_hourly = px.bar(
            hourly_counts,
            x="hour",
            y="count",
            title="Alertes par heure de la journ√©e",
            labels={"hour": "Heure", "count": "Nombre d'alertes"}
        )
        st.plotly_chart(fig_hourly, use_container_width=True)
    
    with col_hist2:
        # Graphique 2: R√©partition par type
        type_counts = df_history["type"].value_counts().reset_index()
        type_counts.columns = ["type", "count"]
        
        fig_types = px.pie(
            type_counts,
            values="count",
            names="type",
            title="R√©partition par type d'alerte",
            hole=0.3
        )
        st.plotly_chart(fig_types, use_container_width=True)
    
    # Graphique 3: √âvolution de la s√©v√©rit√©
    if "severity" in df_history.columns:
        df_history["severity_numeric"] = pd.to_numeric(df_history["severity"], errors='coerce')
        severity_over_time = df_history.groupby("date")["severity_numeric"].mean().reset_index()
        
        fig_severity = px.line(
            severity_over_time,
            x="date",
            y="severity_numeric",
            title="√âvolution de la s√©v√©rit√© moyenne",
            labels={"severity_numeric": "S√©v√©rit√© moyenne", "date": "Date"}
        )
        st.plotly_chart(fig_severity, use_container_width=True)

# Section configuration Kafka (si connect√©)
st.markdown("---")
st.subheader("üîß Configuration Streaming")

tab_kafka, tab_hdfs = st.tabs(["Kafka", "HDFS"])

with tab_kafka:
    st.markdown("""
    **Configuration Kafka pour le streaming:**
    
    - **Topic:** `climate-alerts`
    - **Broker:** `kafka:9092`
    - **Format:** JSON
    - **Consommateurs:** Spark Streaming, Python Consumer
    """)
    
    if st.button("üì° Tester connexion Kafka"):
        try:
            from kafka import KafkaConsumer, KafkaProducer
            producer = KafkaProducer(bootstrap_servers='kafka:9092')
            st.success("‚úÖ Connexion Kafka √©tablie")
            
            # Envoyer un message test
            test_message = {
                "test": True,
                "timestamp": datetime.now().isoformat(),
                "message": "Test depuis Streamlit"
            }
            producer.send('climate-alerts', json.dumps(test_message).encode('utf-8'))
            st.info("üì§ Message test envoy√© au topic 'climate-alerts'")
            
        except Exception as e:
            st.error(f"‚ùå Erreur Kafka: {e}")

with tab_hdfs:
    st.markdown("**Statut HDFS:**")
    
    if hdfs_client.connected:
        st.success("‚úÖ HDFS connect√©")
        
        # Afficher le nombre d'alertes stock√©es
        try:
            alert_files = hdfs_client.list_files("/hadoop-climate-risk/alerts/")
            parquet_files = [f for f in alert_files if f.endswith(".parquet")]
            
            st.metric("Fichiers d'alertes", len(parquet_files))
            
            if parquet_files:
                # Afficher les derniers fichiers
                with st.expander("Derniers fichiers d'alertes"):
                    for file in parquet_files[-5:]:
                        st.write(f"üìÑ {file.split('/')[-1]}")
                        
                        # Bouton pour pr√©visualiser
                        if st.button(f"Aper√ßu {file.split('/')[-1]}", key=f"preview_{file}"):
                            df_preview = hdfs_client.read_parquet_head(file, 5)
                            st.dataframe(df_preview)
        except:
            st.warning("Impossible de lire le dossier des alertes")
    else:
        st.error("‚ùå HDFS non connect√©")

# Auto-refresh
if st.session_state.auto_refresh:
    time.sleep(refresh_interval)
    st.rerun()

# Footer
st.markdown("---")
st.caption("‚ö†Ô∏è **Note:** Les alertes sont simul√©es. Pour des donn√©es r√©elles, configurez les sources USGS/NOAA en temps r√©el.")