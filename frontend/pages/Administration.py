import streamlit as st
import pandas as pd
import numpy as np
import time
from datetime import datetime, timedelta

st.set_page_config(page_title="Administration", page_icon="âš™ï¸", layout="wide")

st.title("âš™ï¸ Administration du DataLake")

st.info("â„¹ï¸ Interface d'administration pour la gestion du DataLake Climat & Risques Naturels")

# Onglets d'administration
tab1, tab2, tab3 = st.tabs(["ğŸ“¥ Ingestion", "ğŸ”§ Traitement", "ğŸ“¤ Export"])

with tab1:
    st.header("ğŸ“¥ Gestion de l'Ingestion")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("ğŸŒ¤ï¸ Ingestion NOAA")
        
        if st.button("ğŸš€ Lancer ingestion NOAA", 
                    use_container_width=True,
                    type="primary"):
            
            with st.spinner("Connexion Ã  l'API NOAA..."):
                # Simulation de progression
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                for i in range(100):
                    time.sleep(0.02)
                    progress_bar.progress(i + 1)
                    
                    if i < 25:
                        status_text.text("ğŸ”Œ Connexion Ã  l'API NOAA...")
                    elif i < 50:
                        status_text.text("ğŸ“¥ TÃ©lÃ©chargement des donnÃ©es...")
                    elif i < 75:
                        status_text.text("ğŸ” Validation des donnÃ©es...")
                    else:
                        status_text.text("ğŸ’¾ Ã‰criture vers HDFS...")
                
                # RÃ©sultats simulÃ©s
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                
                st.success("âœ… Ingestion NOAA terminÃ©e avec succÃ¨s")
                
                st.info(f"""
                **ğŸ“Š RÃ©sultats:**
                - Enregistrements ingÃ©rÃ©s: 1,250
                - PÃ©riode: 30 derniers jours
                - Stations: 15 stations mÃ©tÃ©o
                - Format: Parquet compressÃ©
                - Taille: ~45 MB
                
                **ğŸ—‚ï¸ Chemin HDFS:**
                `/hadoop-climate-risk/raw/noaa/noaa_{timestamp}.parquet`
                
                **âš™ï¸ ParamÃ¨tres:**
                - API: api.weather.gov
                - FrÃ©quence: Quotidienne
                - Compression: Snappy
                - Partitionnement: Par date
                """)
                
                # AperÃ§u des donnÃ©es
                with st.expander("ğŸ‘ï¸ AperÃ§u des donnÃ©es ingÃ©rÃ©es"):
                    # GÃ©nÃ©rer des donnÃ©es de dÃ©monstration
                    dates = pd.date_range('2024-01-01', periods=5)
                    sample_data = pd.DataFrame({
                        'timestamp': dates,
                        'station_id': ['NYC001', 'LAX002', 'CHI003', 'MIA004', 'SEA005'],
                        'temperature_c': [15.2, 18.5, 12.3, 24.1, 10.8],
                        'humidity_pct': [65, 42, 78, 85, 55],
                        'wind_speed_kmh': [12.3, 8.7, 15.2, 5.4, 20.1],
                        'pressure_hpa': [1013.2, 1015.8, 1009.5, 1012.1, 1008.7],
                        'precipitation_mm': [0.0, 2.5, 5.1, 0.3, 8.7]
                    })
                    st.dataframe(sample_data, use_container_width=True)
    
    with col2:
        st.subheader("ğŸŒ‹ Ingestion USGS")
        
        if st.button("ğŸš€ Lancer ingestion USGS", 
                    use_container_width=True,
                    type="primary"):
            
            with st.spinner("Connexion Ã  l'API USGS..."):
                # Simulation de progression
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                for i in range(100):
                    time.sleep(0.02)
                    progress_bar.progress(i + 1)
                    
                    if i < 25:
                        status_text.text("ğŸ”Œ Connexion Ã  l'API USGS...")
                    elif i < 50:
                        status_text.text("ğŸ“¥ TÃ©lÃ©chargement des sÃ©ismes...")
                    elif i < 75:
                        status_text.text("ğŸ” Validation gÃ©ospatiale...")
                    else:
                        status_text.text("ğŸ’¾ Ã‰criture vers HDFS...")
                
                # RÃ©sultats simulÃ©s
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                
                st.success("âœ… Ingestion USGS terminÃ©e avec succÃ¨s")
                
                st.info(f"""
                **ğŸ“Š RÃ©sultats:**
                - SÃ©ismes collectÃ©s: 850
                - PÃ©riode: 7 derniers jours
                - Magnitude minimale: 2.5
                - RÃ©gions: 8 rÃ©gions US
                - Format: Parquet compressÃ©
                - Taille: ~38 MB
                
                **ğŸ—‚ï¸ Chemin HDFS:**
                `/hadoop-climate-risk/raw/usgs/earthquakes_{timestamp}.parquet`
                
                **âš™ï¸ ParamÃ¨tres:**
                - API: earthquake.usgs.gov
                - FrÃ©quence: Temps rÃ©el
                - Compression: Snappy
                - Partitionnement: Par rÃ©gion et date
                """)
                
                # AperÃ§u des donnÃ©es
                with st.expander("ğŸ‘ï¸ AperÃ§u des donnÃ©es ingÃ©rÃ©es"):
                    # GÃ©nÃ©rer des donnÃ©es de dÃ©monstration
                    timestamps = pd.date_range('2024-01-01', periods=5, freq='H')
                    sample_data = pd.DataFrame({
                        'timestamp': timestamps,
                        'magnitude': [4.5, 3.2, 5.1, 2.8, 4.9],
                        'latitude': [34.0522, 36.1699, 37.7749, 40.7128, 47.6062],
                        'longitude': [-118.2437, -115.1398, -122.4194, -74.0060, -122.3321],
                        'depth_km': [10.2, 15.5, 8.7, 22.1, 12.4],
                        'region': ['California', 'Nevada', 'California', 'New York', 'Washington'],
                        'location': ['Los Angeles', 'Las Vegas', 'San Francisco', 'New York', 'Seattle']
                    })
                    st.dataframe(sample_data, use_container_width=True)
    
    # Upload manuel
    st.markdown("---")
    st.subheader("ğŸ“¤ Upload Manuel de Fichiers")
    
    uploaded_file = st.file_uploader(
        "Choisir un fichier Ã  uploader vers HDFS",
        type=['csv', 'json', 'parquet', 'txt', 'zip'],
        help="Formats supportÃ©s: CSV, JSON, Parquet, TXT, ZIP"
    )
    
    if uploaded_file is not None:
        # Afficher les informations du fichier
        file_size_mb = uploaded_file.size / 1024 / 1024
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.success(f"âœ… Fichier dÃ©tectÃ©: **{uploaded_file.name}**")
            
            st.markdown(f"""
            **ğŸ“„ Informations du fichier:**
            - **Nom:** {uploaded_file.name}
            - **Type:** {uploaded_file.type if uploaded_file.type else 'Inconnu'}
            - **Taille:** {file_size_mb:.2f} MB
            - **DerniÃ¨re modification:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
            """)
        
        with col2:
            destination = st.selectbox(
                "ğŸ“ Destination HDFS",
                [
                    "/hadoop-climate-risk/raw/noaa/",
                    "/hadoop-climate-risk/raw/usgs/", 
                    "/hadoop-climate-risk/silver/temp/",
                    "/hadoop-climate-risk/alerts/",
                    "/hadoop-climate-risk/archive/"
                ]
            )
            
            if st.button("ğŸ“¤ Upload vers HDFS", 
                        type="primary",
                        use_container_width=True):
                
                with st.spinner(f"Upload de {uploaded_file.name}..."):
                    # Simulation d'upload
                    upload_progress = st.progress(0)
                    
                    for i in range(100):
                        time.sleep(0.03)
                        upload_progress.progress(i + 1)
                    
                    st.success(f"âœ… Fichier uploadÃ© vers: {destination}{uploaded_file.name}")
                    
                    # AperÃ§u pour les fichiers CSV
                    if uploaded_file.name.endswith('.csv'):
                        try:
                            df_preview = pd.read_csv(uploaded_file)
                            with st.expander("ğŸ‘ï¸ AperÃ§u du fichier"):
                                st.dataframe(df_preview.head(10), use_container_width=True)
                                st.markdown(f"""
                                **ğŸ“Š Statistiques:**
                                - Lignes: {len(df_preview):,}
                                - Colonnes: {len(df_preview.columns)}
                                - Types de donnÃ©es: {df_preview.dtypes.unique()}
                                """)
                        except:
                            st.warning("âš ï¸ Impossible de lire le fichier CSV")

with tab2:
    st.header("ğŸ”§ Traitement des DonnÃ©es")
    
    # Liste des jobs Spark disponibles
    spark_jobs = {
        "ğŸ§¹ Nettoyage ETL (Raw â†’ Silver)": {
            "description": "Nettoyage des donnÃ©es brutes: traitement des valeurs manquantes, suppression des outliers, normalisation des formats",
            "input_path": "/hadoop-climate-risk/raw/",
            "output_path": "/hadoop-climate-risk/silver/",
            "estimated_time": "3-5 minutes",
            "resources": "2 executors, 4GB RAM"
        },
        "ğŸ“Š AgrÃ©gation Quotidienne (Silver â†’ Gold)": {
            "description": "Calcul des statistiques journaliÃ¨res: moyennes, maximums, minimums, totaux par rÃ©gion",
            "input_path": "/hadoop-climate-risk/silver/",
            "output_path": "/hadoop-climate-risk/gold/daily_aggregates/",
            "estimated_time": "2-3 minutes",
            "resources": "1 executor, 2GB RAM"
        },
        "ğŸš¨ DÃ©tection d'Anomalies": {
            "description": "Identification des valeurs aberrantes et patterns inhabituels dans les donnÃ©es",
            "input_path": "/hadoop-climate-risk/silver/",
            "output_path": "/hadoop-climate-risk/gold/anomalies/",
            "estimated_time": "4-6 minutes",
            "resources": "3 executors, 6GB RAM"
        },
        "ğŸ“ˆ Calcul des Tendances (Mensuelles)": {
            "description": "Analyse des tendances Ã  long terme et calcul des indicateurs mensuels",
            "input_path": "/hadoop-climate-risk/gold/daily_aggregates/",
            "output_path": "/hadoop-climate-risk/gold/monthly_trends/",
            "estimated_time": "5-7 minutes",
            "resources": "2 executors, 4GB RAM"
        },
        "ğŸ”— CorrÃ©lation NOAA-USGS": {
            "description": "Calcul des corrÃ©lations entre donnÃ©es mÃ©tÃ©o et donnÃ©es sismiques",
            "input_path": "/hadoop-climate-risk/silver/noaa/, /hadoop-climate-risk/silver/usgs/",
            "output_path": "/hadoop-climate-risk/gold/correlations/",
            "estimated_time": "6-8 minutes",
            "resources": "4 executors, 8GB RAM"
        }
    }
    
    # SÃ©lection du job
    st.subheader("ğŸ¯ SÃ©lection du Job Spark")
    
    selected_job = st.selectbox(
        "Choisir un job Ã  exÃ©cuter",
        list(spark_jobs.keys()),
        format_func=lambda x: f"{x}"
    )
    
    if selected_job:
        job_info = spark_jobs[selected_job]
        
        st.info(f"""
        **ğŸ“‹ Description:** {job_info['description']}
        
        **âš™ï¸ Configuration:**
        - **EntrÃ©e:** {job_info['input_path']}
        - **Sortie:** {job_info['output_path']}
        - **Temps estimÃ©:** {job_info['estimated_time']}
        - **Ressources:** {job_info['resources']}
        
        **ğŸ“Š Impact:**
        - Traite ~15K enregistrements
        - GÃ©nÃ¨re ~500MB de donnÃ©es
        - Met Ã  jour 5-10 tables
        """)
    
    # Configuration avancÃ©e
    with st.expander("âš™ï¸ Configuration AvancÃ©e"):
        col1, col2 = st.columns(2)
        
        with col1:
            executor_memory = st.selectbox(
                "MÃ©moire par executor",
                ["1g", "2g", "4g", "8g"],
                index=1
            )
            
            num_executors = st.slider(
                "Nombre d'executors",
                min_value=1,
                max_value=10,
                value=2
            )
        
        with col2:
            driver_memory = st.selectbox(
                "MÃ©moire du driver",
                ["1g", "2g", "4g", "8g"],
                index=1
            )
            
            partitions = st.slider(
                "Nombre de partitions",
                min_value=10,
                max_value=1000,
                value=100
            )
    
    # Bouton d'exÃ©cution
    if st.button(f"âš¡ ExÃ©cuter le Job: {selected_job}", 
                type="primary",
                use_container_width=True):
        
        with st.spinner(f"ExÃ©cution du job Spark: {selected_job}..."):
            # Simulation d'exÃ©cution
            job_id = f"spark-job-{int(time.time())}"
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            log_container = st.empty()
            
            logs = []
            
            for percent in range(100):
                time.sleep(0.05)
                progress_bar.progress(percent + 1)
                
                # GÃ©nÃ©rer des logs simulÃ©s
                if percent < 20:
                    status_text.text("ğŸš€ Initialisation du cluster Spark...")
                    if percent % 5 == 0:
                        logs.append(f"[INFO] Initializing Spark session with {num_executors} executors")
                
                elif percent < 40:
                    status_text.text("ğŸ“– Lecture des donnÃ©es depuis HDFS...")
                    if percent % 5 == 0:
                        logs.append(f"[INFO] Reading data from: {job_info['input_path']}")
                
                elif percent < 60:
                    status_text.text("âš™ï¸ Traitement des donnÃ©es...")
                    if percent % 5 == 0:
                        logs.append(f"[INFO] Processing {15000 + percent*100} records...")
                
                elif percent < 80:
                    status_text.text("ğŸ’¾ Ã‰criture des rÃ©sultats...")
                    if percent % 5 == 0:
                        logs.append(f"[INFO] Writing results to: {job_info['output_path']}")
                
                else:
                    status_text.text("âœ… Finalisation et nettoyage...")
                    if percent % 5 == 0:
                        logs.append(f"[INFO] Cleaning temporary files...")
                
                # Afficher les derniers logs
                if logs:
                    log_container.text_area("ğŸ“ Logs d'exÃ©cution", 
                                          "\n".join(logs[-10:]), 
                                          height=150)
            
            # RÃ©sultats de l'exÃ©cution
            st.success(f"âœ… Job {selected_job} terminÃ© avec succÃ¨s")
            
            # DÃ©tails d'exÃ©cution
            with st.expander("ğŸ“‹ DÃ©tails d'exÃ©cution", expanded=True):
                execution_time = timedelta(minutes=2, seconds=45)
                end_time = datetime.now()
                start_time = end_time - execution_time
                
                st.code(f"""
                Job Execution Report
                ====================
                
                Job ID: {job_id}
                Job Name: {selected_job}
                
                Status: SUCCEEDED
                Start Time: {start_time.strftime('%Y-%m-%d %H:%M:%S')}
                End Time: {end_time.strftime('%Y-%m-%d %H:%M:%S')}
                Duration: {execution_time}
                
                Configuration:
                - Driver Memory: {driver_memory}
                - Executor Memory: {executor_memory}
                - Number of Executors: {num_executors}
                - Partitions: {partitions}
                - Master: spark://spark-master:7077
                
                Metrics:
                - Records Processed: 15,230
                - Data Read: 1.8 GB
                - Data Written: 450 MB
                - Shuffle Spill: 0 bytes
                - CPU Time: 45.2 minutes
                - Memory Used: 3.2 GB
                
                Output:
                - HDFS Path: {job_info['output_path']}
                - Files Generated: 12 part-*.parquet files
                - Format: Parquet with Snappy compression
                - Size: ~{450 + np.random.randint(-50, 50)} MB
                
                Performance:
                - Read Throughput: 65 MB/s
                - Write Throughput: 28 MB/s
                - Processing Rate: 5,600 records/second
                
                Next Steps:
                - Data available for visualization
                - Update metadata catalog
                - Send completion notification
                """)
            
            # AperÃ§u des rÃ©sultats
            st.subheader("ğŸ“Š AperÃ§u des RÃ©sultats")
            
            if "AgrÃ©gation" in selected_job:
                # GÃ©nÃ©rer des donnÃ©es d'agrÃ©gation simulÃ©es
                result_data = pd.DataFrame({
                    'date': pd.date_range('2024-01-01', periods=7),
                    'avg_temperature_c': np.random.normal(15, 2, 7),
                    'max_temperature_c': np.random.normal(20, 3, 7),
                    'min_temperature_c': np.random.normal(10, 2, 7),
                    'total_precipitation_mm': np.random.exponential(5, 7),
                    'earthquake_count': np.random.poisson(3, 7),
                    'avg_magnitude': np.random.uniform(3.5, 5.5, 7),
                    'max_magnitude': np.random.uniform(4.5, 7.5, 7)
                })
                
                st.dataframe(result_data.round(2), use_container_width=True)
                
                # Graphique des rÃ©sultats
                fig_results = px.line(
                    result_data,
                    x='date',
                    y=['avg_temperature_c', 'max_temperature_c', 'min_temperature_c'],
                    title='TempÃ©ratures JournaliÃ¨res',
                    labels={'value': 'TempÃ©rature (Â°C)', 'variable': 'Type'}
                )
                st.plotly_chart(fig_results, use_container_width=True)
            
            elif "CorrÃ©lation" in selected_job:
                # GÃ©nÃ©rer des donnÃ©es de corrÃ©lation simulÃ©es
                corr_data = pd.DataFrame({
                    'variable_pair': ['TempÃ©rature-Magnitude', 'HumiditÃ©-Profondeur', 
                                     'Pression-FrÃ©quence', 'Vent-Ã‰nergie'],
                    'correlation_coefficient': [0.15, -0.32, 0.08, -0.21],
                    'p_value': [0.03, 0.001, 0.25, 0.05],
                    'significance': ['Faible', 'Forte', 'Non significative', 'ModÃ©rÃ©e']
                })
                
                st.dataframe(corr_data, use_container_width=True)
                
                # Graphique des corrÃ©lations
                fig_corr = px.bar(
                    corr_data,
                    x='variable_pair',
                    y='correlation_coefficient',
                    color='significance',
                    title='Coefficients de CorrÃ©lation',
                    color_discrete_map={
                        'Forte': '#FF0000',
                        'ModÃ©rÃ©e': '#FFA500',
                        'Faible': '#FFFF00',
                        'Non significative': '#808080'
                    }
                )
                st.plotly_chart(fig_corr, use_container_width=True)

with tab3:
    st.header("ğŸ“¤ Export et Rapports")
    
    # Types de rapports disponibles
    report_types = {
        "ğŸ“„ Rapport Climatique Complet": {
            "description": "Analyse dÃ©taillÃ©e des tendances mÃ©tÃ©orologiques sur la derniÃ¨re annÃ©e",
            "format": "PDF (45 pages)",
            "size": "~25 MB",
            "content_sections": [
                "SynthÃ¨se exÃ©cutive",
                "DonnÃ©es et mÃ©thodologie", 
                "Tendances temporelles",
                "Analyse par rÃ©gion",
                "Comparaisons historiques",
                "Recommandations"
            ],
            "generation_time": "30 secondes"
        },
        "ğŸ“„ Analyse Sismique RÃ©gionale": {
            "description": "Ã‰tude approfondie de l'activitÃ© sismique par rÃ©gion gÃ©ographique",
            "format": "PDF (32 pages)",
            "size": "~18 MB", 
            "content_sections": [
                "Carte des risques",
                "Statistiques par rÃ©gion",
                "Analyse des magnitudes",
                "Profondeur des sÃ©ismes",
                "Recommandations de sÃ©curitÃ©"
            ],
            "generation_time": "25 secondes"
        },
        "ğŸ“„ Ã‰tude de CorrÃ©lation NOAA-USGS": {
            "description": "Analyse statistique des corrÃ©lations entre variables climatiques et sismiques",
            "format": "PDF (28 pages)",
            "size": "~15 MB",
            "content_sections": [
                "MÃ©thodologie statistique",
                "Matrices de corrÃ©lation",
                "Tests de significativitÃ©",
                "Visualisations avancÃ©es",
                "Conclusions scientifiques"
            ],
            "generation_time": "35 secondes"
        },
        "ğŸ“Š Dashboard Interactif (HTML)": {
            "description": "Version exportable du dashboard actuel avec interactivitÃ© prÃ©servÃ©e",
            "format": "HTML + JavaScript",
            "size": "~8 MB",
            "content_sections": [
                "Toutes les visualisations",
                "Filtres interactifs",
                "DonnÃ©es embeddÃ©es",
                "Design responsive"
            ],
            "generation_time": "20 secondes"
        },
        "ğŸ’¾ Dataset Complet (CSV/Parquet)": {
            "description": "Export des donnÃ©es nettoyÃ©es et agrÃ©gÃ©es pour analyse externe",
            "format": "CSV, Parquet, JSON",
            "size": "~850 MB",
            "content_sections": [
                "DonnÃ©es NOAA nettoyÃ©es",
                "DonnÃ©es USGS nettoyÃ©es",
                "AgrÃ©gations journaliÃ¨res",
                "MÃ©tadonnÃ©es complÃ¨tes"
            ],
            "generation_time": "45 secondes"
        }
    }
    
    # SÃ©lection du rapport
    st.subheader("ğŸ¯ SÃ©lection du Rapport")
    
    selected_report = st.selectbox(
        "Choisir un rapport Ã  gÃ©nÃ©rer",
        list(report_types.keys()),
        key="report_selector"
    )
    
    if selected_report:
        report_info = report_types[selected_report]
        
        # Afficher les informations du rapport
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.markdown(f"""
            **ğŸ“‹ Description:** {report_info['description']}
            
            **ğŸ“„ Format:** {report_info['format']}
            **ğŸ“Š Taille estimÃ©e:** {report_info['size']}
            **â±ï¸ Temps de gÃ©nÃ©ration:** {report_info['generation_time']}
            
            **ğŸ“‘ Sections incluses:**
            """)
            
            for section in report_info['content_sections']:
                st.markdown(f"- {section}")
        
        with col2:
            # Options d'export
            st.markdown("**âš™ï¸ Options d'export:**")
            
            if "Dataset" in selected_report:
                export_format = st.radio(
                    "Format d'export",
                    ["CSV", "Parquet", "JSON"],
                    horizontal=True
                )
            else:
                export_format = report_info['format'].split(' ')[0]
            
            include_metadata = st.checkbox("Inclure les mÃ©tadonnÃ©es", value=True)
            compress_output = st.checkbox("Compresser le fichier", value=True)
    
    # Bouton de gÃ©nÃ©ration
    if st.button(f"ğŸ”„ GÃ©nÃ©rer le Rapport: {selected_report}", 
                type="primary",
                use_container_width=True):
        
        with st.spinner(f"GÃ©nÃ©ration du {selected_report}..."):
            # Simulation de gÃ©nÃ©ration
            report_id = f"report-{int(time.time())}"
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            for i in range(100):
                time.sleep(0.03)
                progress_bar.progress(i + 1)
                
                if i < 20:
                    status_text.text("ğŸ“– Collecte des donnÃ©es...")
                elif i < 40:
                    status_text.text("ğŸ” Analyse statistique...")
                elif i < 60:
                    status_text.text("ğŸ“Š GÃ©nÃ©ration des visualisations...")
                elif i < 80:
                    status_text.text("ğŸ“„ Formatage du rapport...")
                else:
                    status_text.text("ğŸ’¾ Finalisation de l'export...")
            
            # Message de succÃ¨s
            st.success(f"âœ… {selected_report} gÃ©nÃ©rÃ© avec succÃ¨s")
            
            # Informations sur le fichier gÃ©nÃ©rÃ©
            file_extension = export_format.lower()
            if "PDF" in export_format:
                file_extension = "pdf"
            elif "HTML" in export_format:
                file_extension = "html"
            
            filename = f"{selected_report.lower().replace('ğŸ“„ ', '').replace('ğŸ“Š ', '').replace('ğŸ’¾ ', '').replace(' ', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.{file_extension}"
            
            st.info(f"""
            **ğŸ“„ Fichier gÃ©nÃ©rÃ©:** `{filename}`
            **ğŸ—‚ï¸ Chemin HDFS:** `/hadoop-climate-risk/gold/reports/{datetime.now().strftime('%Y%m%d')}/`
            **ğŸ“ Taille finale:** {report_info['size']}
            **ğŸ•’ GÃ©nÃ©rÃ© le:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
            **ğŸ”§ Options appliquÃ©es:**
            - Format: {export_format}
            - MÃ©tadonnÃ©es: {'Inclues' if include_metadata else 'Exclues'}
            - Compression: {'ActivÃ©e' if compress_output else 'DÃ©sactivÃ©e'}
            """)
            
            # Contenu simulÃ© pour le tÃ©lÃ©chargement
            report_content = f"""
            ============================================
            {selected_report}
            ============================================
            
            GÃ©nÃ©rÃ© le: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
            ID du rapport: {report_id}
            Format d'export: {export_format}
            
            ============================================
            SOMMAIRE EXÃ‰CUTIF
            ============================================
            
            Ce rapport prÃ©sente une analyse complÃ¨te des donnÃ©es collectÃ©es
            dans le DataLake Climat & Risques Naturels.
            
            DonnÃ©es analysÃ©es:
            - PÃ©riode: DerniÃ¨re annÃ©e
            - Sources: NOAA (mÃ©tÃ©o) + USGS (sismique)
            - Enregistrements: ~1.5 million
            - Stations: 15 stations NOAA
            - RÃ©gions sismiques: 8 rÃ©gions US
            
            Principales conclusions:
            1. Tendances climatiques identifiÃ©es
            2. Patterns sismiques dÃ©tectÃ©s
            3. CorrÃ©lations statistiques calculÃ©es
            4. Recommandations opÃ©rationnelles
            
            ============================================
            MÃ‰THODOLOGIE
            ============================================
            
            MÃ©thodes statistiques utilisÃ©es:
            - Analyse de sÃ©ries temporelles
            - Calcul de corrÃ©lations
            - Tests de significativitÃ©
            - Visualisations avancÃ©es
            
            Outils:
            - Apache Spark pour le traitement
            - Python pour l'analyse
            - Plotly pour les visualisations
            
            ============================================
            DONNÃ‰ES TECHNIQUES
            ============================================
            
            MÃ©triques de qualitÃ©:
            - ComplÃ©tude: 98.7%
            - Exactitude: 99.2%
            - Consistance: 97.8%
            - ActualitÃ©: 99.9%
            
            Limitations:
            - DonnÃ©es simulÃ©es pour dÃ©monstration
            - En production: sources rÃ©elles temps rÃ©el
            
            ============================================
            CONTACT ET SUPPORT
            ============================================
            
            Pour plus d'informations:
            - Email: datalake@climate-risks.com
            - Documentation: https://docs.climate-risks.com
            - Support technique: support@climate-risks.com
            
            Â© 2024 DataLake Climat & Risques Naturels
            """
            
            # Bouton de tÃ©lÃ©chargement
            st.download_button(
                label=f"ğŸ“¥ TÃ©lÃ©charger {selected_report}",
                data=report_content,
                file_name=filename,
                mime="text/plain" if file_extension == "txt" else 
                      "application/pdf" if file_extension == "pdf" else
                      "text/html" if file_extension == "html" else
                      "text/csv" if file_extension == "csv" else
                      "application/json" if file_extension == "json" else
                      "application/octet-stream",
                help=f"Cliquez pour tÃ©lÃ©charger le {selected_report}",
                use_container_width=True
            )
            
            # Options supplÃ©mentaires
            st.markdown("---")
            st.subheader("ğŸ”„ Actions SupplÃ©mentaires")
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                if st.button("ğŸ“§ Envoyer par email", use_container_width=True):
                    st.info("âœ… Rapport envoyÃ© Ã  l'adresse enregistrÃ©e")
            
            with col2:
                if st.button("ğŸ—‚ï¸ Archiver dans HDFS", use_container_width=True):
                    st.info("âœ… Rapport archivÃ© dans HDFS pour conservation")
            
            with col3:
                if st.button("ğŸ“Š Ajouter au catalogue", use_container_width=True):
                    st.info("âœ… Rapport ajoutÃ© au catalogue de donnÃ©es")

# Footer
st.markdown("---")
st.caption("""
**âš ï¸ Note:** Cette interface d'administration est une simulation. 
En environnement de production, toutes les opÃ©rations seraient exÃ©cutÃ©es 
sur un cluster Spark/Hadoop rÃ©el avec connexion aux APIs NOAA et USGS.
""")