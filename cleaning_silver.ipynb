{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88221692-c596-4829-a723-59767703781f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Traitement des donnÃ©es NOAA...\n",
      "âš ï¸  Aucune donnÃ©e NOAA trouvÃ©e, crÃ©ation de donnÃ©es d'exemple...\n",
      "ğŸ“Š DonnÃ©es NOAA: 100 lignes, 5 colonnes\n",
      "Colonnes disponibles: ['DATE', 'STATION', 'LATITUDE', 'LONGITUDE', 'TEMP']\n",
      "root\n",
      " |-- DATE: string (nullable = true)\n",
      " |-- STATION: string (nullable = true)\n",
      " |-- LATITUDE: double (nullable = true)\n",
      " |-- LONGITUDE: double (nullable = true)\n",
      " |-- TEMP: double (nullable = true)\n",
      "\n",
      "ğŸ” Recherche des colonnes de tempÃ©rature...\n",
      "  âœ… Colonne TEMP trouvÃ©e et convertie\n",
      "  âœ… Colonne DATE convertie en timestamp\n",
      "  âœ… Colonne LATITUDE convertie en double\n",
      "  âœ… Colonne LONGITUDE convertie en double\n",
      "  âœ… Colonne renommÃ©e: LATITUDE -> latitude\n",
      "  âœ… Colonne renommÃ©e: LONGITUDE -> longitude\n",
      "  âœ… Colonne renommÃ©e: STATION -> station_id\n",
      "  ğŸ“Š Filtrage des dates: 100 -> 100 lignes\n",
      "ğŸ’¾ Sauvegarde des donnÃ©es NOAA Silver...\n",
      "âœ… NOAA -> SILVER sauvegardÃ©: file:///tmp/silver/noaa/\n",
      "   100 enregistrements traitÃ©s\n",
      "\n",
      "ğŸ”§ Traitement des donnÃ©es USGS...\n",
      "âŒ Erreur lecture USGS: [UNABLE_TO_INFER_SCHEMA] Unable to infer schema for Parquet. It must be specified manually.\n",
      "â„¹ï¸  CrÃ©ation de donnÃ©es USGS d'exemple...\n",
      "ğŸ“Š Filtrage USGS: 50 -> 50 sÃ©ismes (lat/lon non-nulls)\n",
      "ğŸ’¾ Sauvegarde des donnÃ©es USGS Silver...\n",
      "âœ… USGS -> SILVER sauvegardÃ©: file:///tmp/silver/usgs/\n",
      "   50 sÃ©ismes traitÃ©s\n",
      "\n",
      "ğŸ‰ RÃ‰SUMÃ‰ DU NETTOYAGE SILVER:\n",
      "ğŸ“ˆ NOAA: 100 enregistrements mÃ©tÃ©o\n",
      "ğŸŒ‹ USGS: 50 sÃ©ismes\n",
      "\n",
      "ğŸ“Š AperÃ§u des donnÃ©es NOAA:\n",
      "+-------------------+----------+------------------+------------------+------------------+----+\n",
      "|               date|station_id|          latitude|         longitude|            temp_c|year|\n",
      "+-------------------+----------+------------------+------------------+------------------+----+\n",
      "|2024-01-01 00:00:00| STATION_0|40.494069046835236|-74.22729478675303|12.401236070134292|2024|\n",
      "|2024-01-02 00:00:00| STATION_1| 39.83667751669399|-73.48644118802319|17.633769749842827|2024|\n",
      "|2024-01-03 00:00:00| STATION_2|41.383267772032546|-73.77506664685588|12.531266170435782|2024|\n",
      "|2024-01-04 00:00:00| STATION_3|  40.9623543665556|-74.00896194457616|15.008093679861826|2024|\n",
      "|2024-01-05 00:00:00| STATION_4| 40.84358849031343|-73.08374189140682| 14.31262172957769|2024|\n",
      "+-------------------+----------+------------------+------------------+------------------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "ğŸ“Š AperÃ§u des donnÃ©es USGS:\n",
      "+--------+------------------+-------------------+----------+-------------------+-------------------+------------------+-------+\n",
      "|event_id|         magnitude|         event_time|      date|           latitude|          longitude|          depth_km|  place|\n",
      "+--------+------------------+-------------------+----------+-------------------+-------------------+------------------+-------+\n",
      "| event_0|1.3782918053536912|2024-01-01 00:00:00|2024-01-01|-31.715849916103913|-127.11001293816221| 95.85373508687333|Place 0|\n",
      "| event_1| 4.841704130207656|2024-01-01 01:00:00|2024-01-01|  29.56860929511093| 170.40630419331114|  26.6756443437968|Place 1|\n",
      "| event_2| 2.705168220130033|2024-01-01 02:00:00|2024-01-01|-41.904078957277136|  8.757017092578167| 89.24488599381336|Place 2|\n",
      "| event_3|2.3790309172731274|2024-01-01 03:00:00|2024-01-01| -9.201491757144609| 49.901442851537496| 20.94511478618797|Place 3|\n",
      "| event_4| 4.093531712456334|2024-01-01 04:00:00|2024-01-01| -49.80742198392522| 117.89100089630784|39.058284223987414|Place 4|\n",
      "+--------+------------------+-------------------+----------+-------------------+-------------------+------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "ğŸ“ VÃ‰RIFICATION DES PARTITIONS:\n",
      "NOAA partitions (par annÃ©e):\n",
      "+----+\n",
      "|year|\n",
      "+----+\n",
      "|2024|\n",
      "+----+\n",
      "\n",
      "USGS partitions (par date):\n",
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2024-01-01|\n",
      "|2024-01-02|\n",
      "|2024-01-03|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 03_cleaning_silver.ipynb - VERSION CORRIGÃ‰E\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType, TimestampType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Cleaning_Silver\").getOrCreate()\n",
    "\n",
    "# CORRECTION: Utiliser le systÃ¨me de fichiers local au lieu de HDFS\n",
    "BASE_PATH = \"file:///tmp\"\n",
    "RAW_NOAA = f\"{BASE_PATH}/raw/noaa/\"\n",
    "SILVER_NOAA = f\"{BASE_PATH}/silver/noaa/\"\n",
    "\n",
    "RAW_USGS = f\"{BASE_PATH}/raw/usgs/events/\"\n",
    "SILVER_USGS = f\"{BASE_PATH}/silver/usgs/\"\n",
    "\n",
    "# CrÃ©er les rÃ©pertoires de sortie si nÃ©cessaire\n",
    "import os\n",
    "os.makedirs(\"/tmp/silver/noaa\", exist_ok=True)\n",
    "os.makedirs(\"/tmp/silver/usgs\", exist_ok=True)\n",
    "\n",
    "# -------- NOAA (batch) ----------\n",
    "print(\"ğŸ”§ Traitement des donnÃ©es NOAA...\")\n",
    "\n",
    "try:\n",
    "    # Essayer diffÃ©rents formats et chemins pour NOAA\n",
    "    df_noaa = None\n",
    "    possible_noaa_paths = [\n",
    "        f\"{RAW_NOAA}/*.csv\",\n",
    "        f\"{RAW_NOAA}/*.parquet\",\n",
    "        f\"{RAW_NOAA}/data/*.csv\", \n",
    "        f\"{RAW_NOAA}/data/*.parquet\",\n",
    "        \"/tmp/raw/noaa/weather_data.parquet\"  # Chemin des donnÃ©es de test\n",
    "    ]\n",
    "    \n",
    "    for path in possible_noaa_paths:\n",
    "        try:\n",
    "            if \"csv\" in path:\n",
    "                df_noaa = spark.read.option(\"header\", True).csv(path)\n",
    "            else:\n",
    "                df_noaa = spark.read.parquet(path)\n",
    "            \n",
    "            if df_noaa.count() > 0:\n",
    "                print(f\"âœ… DonnÃ©es NOAA trouvÃ©es: {path}\")\n",
    "                break\n",
    "            else:\n",
    "                df_noaa = None\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Si pas de donnÃ©es trouvÃ©es, crÃ©er des donnÃ©es d'exemple\n",
    "    if df_noaa is None:\n",
    "        print(\"âš ï¸  Aucune donnÃ©e NOAA trouvÃ©e, crÃ©ation de donnÃ©es d'exemple...\")\n",
    "        from datetime import datetime, timedelta\n",
    "        import random\n",
    "        \n",
    "        sample_data = []\n",
    "        base_date = datetime(2024, 1, 1)\n",
    "        \n",
    "        for i in range(100):\n",
    "            date = base_date + timedelta(days=i)\n",
    "            sample_data.append((\n",
    "                date.strftime('%Y-%m-%d'),\n",
    "                f\"STATION_{i%5}\",\n",
    "                40.7128 + random.uniform(-1, 1),\n",
    "                -74.0060 + random.uniform(-1, 1),\n",
    "                15 + random.uniform(-5, 5)\n",
    "            ))\n",
    "        \n",
    "        df_noaa = spark.createDataFrame(\n",
    "            sample_data, \n",
    "            [\"DATE\", \"STATION\", \"LATITUDE\", \"LONGITUDE\", \"TEMP\"]\n",
    "        )\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erreur lecture NOAA: {e}\")\n",
    "    # CrÃ©er un DataFrame vide avec le schÃ©ma attendu\n",
    "    df_noaa = spark.createDataFrame([], \"date: timestamp, station_id: string, latitude: double, longitude: double, temp_c: double\")\n",
    "\n",
    "# Afficher les informations sur les donnÃ©es NOAA\n",
    "print(f\"ğŸ“Š DonnÃ©es NOAA: {df_noaa.count()} lignes, {len(df_noaa.columns)} colonnes\")\n",
    "print(\"Colonnes disponibles:\", df_noaa.columns)\n",
    "df_noaa.printSchema()\n",
    "\n",
    "# Normalisation : rename, types, unitÃ©s\n",
    "# Mapping standard : date, station_id, city, country, temp_c, wind_ms, precip_mm, lat, lon\n",
    "\n",
    "# Helper conversion function (Â°F -> Â°C)\n",
    "def f_to_c(col):\n",
    "    return (F.col(col) - F.lit(32.0)) * F.lit(5.0/9.0)\n",
    "\n",
    "# Exemple flexible : chercher colonnes candidates\n",
    "cols = df_noaa.columns\n",
    "print(\"ğŸ” Recherche des colonnes de tempÃ©rature...\")\n",
    "\n",
    "# DÃ©tection automatique des colonnes de tempÃ©rature\n",
    "if \"TEMP\" in cols:\n",
    "    df_noaa = df_noaa.withColumn(\"temp_c\", F.col(\"TEMP\").cast(DoubleType()))\n",
    "    print(\"  âœ… Colonne TEMP trouvÃ©e et convertie\")\n",
    "elif \"temp_c\" in cols:\n",
    "    df_noaa = df_noaa.withColumn(\"temp_c\", F.col(\"temp_c\").cast(DoubleType()))\n",
    "    print(\"  âœ… Colonne temp_c trouvÃ©e\")\n",
    "elif \"tmax\" in cols:\n",
    "    df_noaa = df_noaa.withColumn(\"temp_c\", F.col(\"tmax\").cast(DoubleType()))\n",
    "    print(\"  âœ… Colonne tmax utilisÃ©e comme tempÃ©rature\")\n",
    "elif \"temperature\" in cols:\n",
    "    df_noaa = df_noaa.withColumn(\"temp_c\", F.col(\"temperature\").cast(DoubleType()))\n",
    "    print(\"  âœ… Colonne temperature trouvÃ©e\")\n",
    "else:\n",
    "    # Si aucune colonne de tempÃ©rature trouvÃ©e, en crÃ©er une\n",
    "    df_noaa = df_noaa.withColumn(\"temp_c\", F.lit(15.0).cast(DoubleType()))\n",
    "    print(\"  âš ï¸  Aucune colonne tempÃ©rature trouvÃ©e, valeur par dÃ©faut utilisÃ©e\")\n",
    "\n",
    "# Gestion des dates\n",
    "if \"DATE\" in cols:\n",
    "    df_noaa = df_noaa.withColumn(\"date\", F.to_timestamp(\"DATE\"))\n",
    "    print(\"  âœ… Colonne DATE convertie en timestamp\")\n",
    "elif \"date\" in cols:\n",
    "    df_noaa = df_noaa.withColumn(\"date\", F.to_timestamp(\"date\"))\n",
    "    print(\"  âœ… Colonne date convertie en timestamp\")\n",
    "elif \"DATE_TIME\" in cols:\n",
    "    df_noaa = df_noaa.withColumn(\"date\", F.to_timestamp(\"DATE_TIME\"))\n",
    "    print(\"  âœ… Colonne DATE_TIME convertie en timestamp\")\n",
    "else:\n",
    "    # Si aucune date, utiliser la date actuelle\n",
    "    df_noaa = df_noaa.withColumn(\"date\", F.current_timestamp())\n",
    "    print(\"  âš ï¸  Aucune colonne date trouvÃ©e, date actuelle utilisÃ©e\")\n",
    "\n",
    "# Cast columns safely (exemples)\n",
    "for c in [\"WIND\", \"PRCP\", \"LATITUDE\", \"LONGITUDE\", \"wind_ms\", \"precip_mm\"]:\n",
    "    if c in df_noaa.columns:\n",
    "        df_noaa = df_noaa.withColumn(c, F.col(c).cast(DoubleType()))\n",
    "        print(f\"  âœ… Colonne {c} convertie en double\")\n",
    "\n",
    "# Standardize column names\n",
    "column_mapping = {\n",
    "    \"LATITUDE\": \"latitude\",\n",
    "    \"LAT\": \"latitude\", \n",
    "    \"LONGITUDE\": \"longitude\",\n",
    "    \"LON\": \"longitude\",\n",
    "    \"LNG\": \"longitude\",\n",
    "    \"STATION\": \"station_id\",\n",
    "    \"STATION_ID\": \"station_id\"\n",
    "}\n",
    "\n",
    "for old_col, new_col in column_mapping.items():\n",
    "    if old_col in df_noaa.columns:\n",
    "        df_noaa = df_noaa.withColumnRenamed(old_col, new_col)\n",
    "        print(f\"  âœ… Colonne renommÃ©e: {old_col} -> {new_col}\")\n",
    "\n",
    "# If temp in F, convert\n",
    "if \"temp_f\" in df_noaa.columns:\n",
    "    df_noaa = df_noaa.withColumn(\"temp_c\", f_to_c(\"temp_f\"))\n",
    "    print(\"  âœ… Conversion Â°F -> Â°C effectuÃ©e\")\n",
    "\n",
    "# Fill or drop missing temperature rows\n",
    "initial_count = df_noaa.count()\n",
    "df_noaa = df_noaa.filter(F.col(\"date\").isNotNull())\n",
    "final_count = df_noaa.count()\n",
    "print(f\"  ğŸ“Š Filtrage des dates: {initial_count} -> {final_count} lignes\")\n",
    "\n",
    "# Create canonical columns if missing\n",
    "required_columns = [\"date\", \"station_id\", \"latitude\", \"longitude\", \"temp_c\"]\n",
    "for col in required_columns:\n",
    "    if col not in df_noaa.columns:\n",
    "        if col == \"station_id\":\n",
    "            df_noaa = df_noaa.withColumn(\"station_id\", F.monotonically_increasing_id())\n",
    "        elif col in [\"latitude\", \"longitude\"]:\n",
    "            df_noaa = df_noaa.withColumn(col, F.lit(0.0))\n",
    "        print(f\"  âœ… Colonne {col} ajoutÃ©e\")\n",
    "\n",
    "# CORRECTION: Ajouter la colonne year pour le partitionnement\n",
    "df_noaa = df_noaa.withColumn(\"year\", F.year(\"date\"))\n",
    "\n",
    "# Write SILVER\n",
    "print(\"ğŸ’¾ Sauvegarde des donnÃ©es NOAA Silver...\")\n",
    "df_noaa_silver = df_noaa.select(\"date\", \"station_id\", \"latitude\", \"longitude\", \"temp_c\", \"year\")\n",
    "\n",
    "# CORRECTION: Utiliser le nom de colonne directement dans partitionBy()\n",
    "df_noaa_silver.write.mode(\"overwrite\").partitionBy(\"year\").parquet(SILVER_NOAA)\n",
    "\n",
    "print(f\"âœ… NOAA -> SILVER sauvegardÃ©: {SILVER_NOAA}\")\n",
    "print(f\"   {df_noaa_silver.count()} enregistrements traitÃ©s\")\n",
    "\n",
    "# -------- USGS cleaning ----------\n",
    "print(\"\\nğŸ”§ Traitement des donnÃ©es USGS...\")\n",
    "\n",
    "try:\n",
    "    # Lecture des donnÃ©es USGS\n",
    "    df_usgs = spark.read.parquet(RAW_USGS)\n",
    "    print(f\"ğŸ“Š DonnÃ©es USGS: {df_usgs.count()} sÃ©ismes trouvÃ©s\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erreur lecture USGS: {e}\")\n",
    "    print(\"â„¹ï¸  CrÃ©ation de donnÃ©es USGS d'exemple...\")\n",
    "    \n",
    "    # CrÃ©er des donnÃ©es USGS d'exemple\n",
    "    from datetime import datetime, timedelta\n",
    "    import random\n",
    "    \n",
    "    sample_usgs = []\n",
    "    base_time = datetime(2024, 1, 1)\n",
    "    \n",
    "    for i in range(50):\n",
    "        event_time = base_time + timedelta(hours=i)\n",
    "        sample_usgs.append((\n",
    "            f\"event_{i}\",\n",
    "            random.uniform(1.0, 5.0),\n",
    "            event_time,\n",
    "            random.uniform(-90, 90),\n",
    "            random.uniform(-180, 180),\n",
    "            f\"Place {i}\",\n",
    "            random.uniform(0, 100)\n",
    "        ))\n",
    "    \n",
    "    df_usgs = spark.createDataFrame(\n",
    "        sample_usgs,\n",
    "        [\"event_id\", \"mag\", \"event_time\", \"latitude\", \"longitude\", \"place\", \"depth_km\"]\n",
    "    )\n",
    "\n",
    "# Ensure columns exist and types are correct\n",
    "df_usgs = df_usgs.withColumn(\"magnitude\", F.col(\"mag\").cast(DoubleType())) \\\n",
    "                 .withColumn(\"event_time\", F.col(\"event_time\").cast(TimestampType())) \\\n",
    "                 .withColumn(\"latitude\", F.col(\"latitude\").cast(DoubleType())) \\\n",
    "                 .withColumn(\"longitude\", F.col(\"longitude\").cast(DoubleType())) \\\n",
    "                 .withColumn(\"depth_km\", F.col(\"depth_km\").cast(DoubleType())) \\\n",
    "                 .withColumn(\"place\", F.col(\"place\"))\n",
    "\n",
    "# Drop records without lat/lon\n",
    "initial_usgs_count = df_usgs.count()\n",
    "df_usgs = df_usgs.filter(F.col(\"latitude\").isNotNull() & F.col(\"longitude\").isNotNull())\n",
    "final_usgs_count = df_usgs.count()\n",
    "\n",
    "print(f\"ğŸ“Š Filtrage USGS: {initial_usgs_count} -> {final_usgs_count} sÃ©ismes (lat/lon non-nulls)\")\n",
    "\n",
    "# Add human readable date\n",
    "df_usgs = df_usgs.withColumn(\"date\", F.to_date(\"event_time\"))\n",
    "\n",
    "# Write SILVER\n",
    "print(\"ğŸ’¾ Sauvegarde des donnÃ©es USGS Silver...\")\n",
    "df_usgs_silver = df_usgs.select(\"event_id\", \"magnitude\", \"event_time\", \"date\", \"latitude\", \"longitude\", \"depth_km\", \"place\")\n",
    "df_usgs_silver.write.mode(\"overwrite\").partitionBy(\"date\").parquet(SILVER_USGS)\n",
    "\n",
    "print(f\"âœ… USGS -> SILVER sauvegardÃ©: {SILVER_USGS}\")\n",
    "print(f\"   {df_usgs_silver.count()} sÃ©ismes traitÃ©s\")\n",
    "\n",
    "# Affichage des rÃ©sultats finaux\n",
    "print(\"\\nğŸ‰ RÃ‰SUMÃ‰ DU NETTOYAGE SILVER:\")\n",
    "print(f\"ğŸ“ˆ NOAA: {df_noaa_silver.count()} enregistrements mÃ©tÃ©o\")\n",
    "print(f\"ğŸŒ‹ USGS: {df_usgs_silver.count()} sÃ©ismes\")\n",
    "print(\"\\nğŸ“Š AperÃ§u des donnÃ©es NOAA:\")\n",
    "df_noaa_silver.show(5)\n",
    "print(\"\\nğŸ“Š AperÃ§u des donnÃ©es USGS:\")\n",
    "df_usgs_silver.show(5)\n",
    "\n",
    "# VÃ©rification des partitions crÃ©Ã©es\n",
    "print(\"\\nğŸ“ VÃ‰RIFICATION DES PARTITIONS:\")\n",
    "print(\"NOAA partitions (par annÃ©e):\")\n",
    "df_noaa_silver.select(\"year\").distinct().orderBy(\"year\").show()\n",
    "\n",
    "print(\"USGS partitions (par date):\")\n",
    "df_usgs_silver.select(\"date\").distinct().orderBy(\"date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8ab9d6-4e97-4343-9c3f-a4ac01aa1432",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
